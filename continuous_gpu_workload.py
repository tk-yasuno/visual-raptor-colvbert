#!/usr/bin/env python3
"""
Continuous GPU Workload for Visual RAPTOR ColBERT
Visual RAPTOR ColBERTã‚·ã‚¹ãƒ†ãƒ ã§GPUã‚’ç¶™ç¶šçš„ã«ç¨¼åƒã•ã›ã‚‹
"""

import torch
import torch.nn.functional as F
import numpy as np
import time
import threading
from datetime import datetime
from typing import List
import queue
import json
import signal
import sys

class ContinuousGPUWorkload:
    """ç¶™ç¶šçš„GPUå‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.running = True
        self.workload_queue = queue.Queue()
        self.results_queue = queue.Queue()
        
        # GPUè¨­å®š
        if torch.cuda.is_available():
            print(f"ğŸš€ GPU Workload Manager Started")
            print(f"Device: {torch.cuda.get_device_name(0)}")
            print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            print("âŒ CUDA not available")
    
    def signal_handler(self, signum, frame):
        """ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
        print("\nğŸ›‘ Stopping GPU workload...")
        self.running = False
        sys.exit(0)
    
    def embedding_workload(self):
        """åŸ‹ã‚è¾¼ã¿å‡¦ç†ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰"""
        print("ğŸ§  Starting continuous embedding workload...")
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        vocab_size = 30000
        embedding_dim = 768
        model = torch.nn.Sequential(
            torch.nn.Embedding(vocab_size, embedding_dim),
            torch.nn.LayerNorm(embedding_dim),
            torch.nn.Linear(embedding_dim, embedding_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(embedding_dim, embedding_dim)
        ).to(self.device)
        
        batch_count = 0
        
        while self.running:
            try:
                # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                batch_size = np.random.randint(64, 256)
                seq_length = np.random.randint(128, 512)
                
                input_ids = torch.randint(0, vocab_size, (batch_size, seq_length), device=self.device)
                
                # å‰å‘ãå‡¦ç†
                with torch.no_grad():
                    embeddings = model(input_ids)
                    
                    # è¿½åŠ å‡¦ç†
                    attention_weights = torch.softmax(
                        torch.matmul(embeddings, embeddings.transpose(-2, -1)) / np.sqrt(embedding_dim),
                        dim=-1
                    )
                    
                    output = torch.matmul(attention_weights, embeddings)
                    
                batch_count += 1
                
                if batch_count % 100 == 0:
                    memory_used = torch.cuda.memory_allocated() / 1e9
                    print(f"   Embedding batches: {batch_count}, Memory: {memory_used:.2f} GB")
                
                time.sleep(0.01)  # çŸ­ã„å¾…æ©Ÿ
                
            except Exception as e:
                print(f"Error in embedding workload: {e}")
                time.sleep(1)
    
    def search_workload(self):
        """æ¤œç´¢å‡¦ç†ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰"""
        print("ğŸ” Starting continuous search workload...")
        
        # æ–‡æ›¸åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        num_docs = 50000
        embedding_dim = 768
        
        doc_embeddings = torch.randn(num_docs, embedding_dim, device=self.device, dtype=torch.float32)
        doc_embeddings = F.normalize(doc_embeddings, p=2, dim=1)
        
        search_count = 0
        
        while self.running:
            try:
                # ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ã‚¨ãƒªç”Ÿæˆ
                num_queries = np.random.randint(5, 20)
                query_embeddings = torch.randn(num_queries, embedding_dim, device=self.device, dtype=torch.float32)
                query_embeddings = F.normalize(query_embeddings, p=2, dim=1)
                
                # ãƒãƒƒãƒæ¤œç´¢å®Ÿè¡Œ
                similarities = torch.mm(query_embeddings, doc_embeddings.t())
                top_k = np.random.randint(10, 100)
                top_scores, top_indices = torch.topk(similarities, k=top_k, dim=1)
                
                search_count += num_queries
                
                if search_count % 500 == 0:
                    memory_used = torch.cuda.memory_allocated() / 1e9
                    print(f"   Search queries: {search_count}, Memory: {memory_used:.2f} GB")
                
                time.sleep(0.05)  # çŸ­ã„å¾…æ©Ÿ
                
            except Exception as e:
                print(f"Error in search workload: {e}")
                time.sleep(1)
    
    def matrix_workload(self):
        """è¡Œåˆ—è¨ˆç®—ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰"""
        print("ğŸ”¢ Starting continuous matrix workload...")
        
        operation_count = 0
        
        while self.running:
            try:
                # ãƒ©ãƒ³ãƒ€ãƒ ã‚µã‚¤ã‚ºã®è¡Œåˆ—ç”Ÿæˆ
                size = np.random.randint(1000, 4000)
                
                a = torch.randn(size, size, device=self.device, dtype=torch.float32)
                b = torch.randn(size, size, device=self.device, dtype=torch.float32)
                
                # è¡Œåˆ—æ¼”ç®—
                c = torch.mm(a, b)
                c = F.relu(c)
                c = torch.mm(c, a.t())
                c = F.normalize(c, p=2, dim=1)
                
                operation_count += 1
                
                if operation_count % 50 == 0:
                    memory_used = torch.cuda.memory_allocated() / 1e9
                    print(f"   Matrix operations: {operation_count}, Memory: {memory_used:.2f} GB")
                
                time.sleep(0.1)  # çŸ­ã„å¾…æ©Ÿ
                
            except Exception as e:
                print(f"Error in matrix workload: {e}")
                time.sleep(1)
    
    def monitor_gpu(self):
        """GPUç›£è¦–"""
        print("ğŸ“Š Starting GPU monitoring...")
        
        while self.running:
            try:
                import subprocess
                result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu,power.draw'], 
                                      capture_output=True, text=True, timeout=5)
                
                if result.returncode == 0:
                    values = result.stdout.strip().split(', ')
                    if len(values) >= 4:
                        gpu_util = values[0].replace(' %', '')
                        mem_used = values[1].replace(' MiB', '')
                        mem_total = values[2].replace(' MiB', '')
                        temp = values[3].replace(' C', '')
                        
                        timestamp = datetime.now().strftime('%H:%M:%S')
                        print(f"ğŸ¯ [{timestamp}] GPU: {gpu_util}%, Memory: {mem_used}/{mem_total}MB, Temp: {temp}Â°C")
                
                time.sleep(10)  # 10ç§’é–“éš”ã§ç›£è¦–
                
            except Exception as e:
                print(f"Error in GPU monitoring: {e}")
                time.sleep(10)
    
    def run_continuous_workload(self):
        """ç¶™ç¶šçš„ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ"""
        if not torch.cuda.is_available():
            print("âŒ CUDA not available")
            return
        
        print("="*80)
        print("ğŸš€ Continuous GPU Workload for Visual RAPTOR ColBERT")
        print("="*80)
        
        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
        signal.signal(signal.SIGINT, self.signal_handler)
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        threads = [
            threading.Thread(target=self.embedding_workload, daemon=True),
            threading.Thread(target=self.search_workload, daemon=True),
            threading.Thread(target=self.matrix_workload, daemon=True),
            threading.Thread(target=self.monitor_gpu, daemon=True)
        ]
        
        for thread in threads:
            thread.start()
        
        print("âœ… All GPU workloads started")
        print("Press Ctrl+C to stop")
        print("="*80)
        
        try:
            while self.running:
                time.sleep(1)
        except KeyboardInterrupt:
            print("\nğŸ›‘ Stopping workloads...")
            self.running = False
        
        print("ğŸ‘‹ GPU workloads stopped")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    workload_manager = ContinuousGPUWorkload()
    workload_manager.run_continuous_workload()


if __name__ == "__main__":
    main()