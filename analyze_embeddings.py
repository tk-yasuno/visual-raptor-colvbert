"""
åŸ‹ã‚è¾¼ã¿ã®è©³ç´°åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ColVBERT vs ColModernVBERT ã®åŸ‹ã‚è¾¼ã¿ç‰¹æ€§ã‚’èª¿æŸ»
"""

import os
import sys
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from pathlib import Path

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from visual_raptor_colbert import VisualRAPTORColBERT
from langchain_ollama import OllamaEmbeddings, ChatOllama
from jina_vdr_benchmark import DisasterDocumentGenerator

print("=" * 80)
print("åŸ‹ã‚è¾¼ã¿ç‰¹æ€§åˆ†æ")
print("=" * 80)

# Ollamaãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
embeddings_model = OllamaEmbeddings(
    model="mxbai-embed-large",
    base_url="http://localhost:11434"
)

llm = ChatOllama(
    model="granite-code:8b",
    base_url="http://localhost:11434",
    temperature=0.0,
    request_timeout=600.0
)

# ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
print("\n[1/5] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ...")
doc_generator = DisasterDocumentGenerator()
documents = doc_generator.create_synthetic_documents(num_documents=10)

# ç”»åƒç”Ÿæˆ
images_dir = Path("data/embedding_analysis")
images_dir.mkdir(parents=True, exist_ok=True)

test_images = []
test_texts = []

for i, doc in enumerate(documents):
    img = Image.new('RGB', (640, 480), color=(255, 255, 255))
    img_path = images_dir / f"test_{i}.png"
    img.save(img_path)
    test_images.append(Image.open(img_path).convert('RGB'))
    test_texts.append(doc['content'][:500])

print(f"âœ… {len(test_images)}å€‹ã®ãƒ†ã‚¹ãƒˆç”»åƒ/ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†")

# ColVBERTåˆæœŸåŒ–
print("\n[2/5] ColVBERTåˆæœŸåŒ–...")
colbert_config = {
    'encoder_type': 'standard',
    'embedding_dim': 768,
    'use_cross_attention': False
}

colbert_system = VisualRAPTORColBERT(
    embeddings_model=embeddings_model,
    llm=llm,
    use_modern_vbert=False,
    colbert_config=colbert_config
)

# ColModernVBERTåˆæœŸåŒ–
print("\n[3/5] ColModernVBERTåˆæœŸåŒ–...")
modern_config = {
    'encoder_type': 'modern',
    'embedding_dim': 768,
    'use_cross_attention': True
}

modern_system = VisualRAPTORColBERT(
    embeddings_model=embeddings_model,
    llm=llm,
    use_modern_vbert=True,
    colbert_config=modern_config
)

# ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
print("\n[4/5] ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ...")

with torch.no_grad():
    # ColVBERT
    colbert_text_emb = colbert_system.colbert_encoder.encode_text(test_texts)
    colbert_img_emb = colbert_system.colbert_encoder.encode_image(test_images)
    
    # ColModernVBERT
    modern_text_emb = modern_system.colbert_encoder.encode_text(test_texts)
    modern_img_emb = modern_system.colbert_encoder.encode_image(test_images)

print("âœ… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Œäº†")

# çµ±è¨ˆåˆ†æ
print("\n[5/5] çµ±è¨ˆåˆ†æ...")

def analyze_embeddings(embeddings, name):
    """åŸ‹ã‚è¾¼ã¿ã®çµ±è¨ˆã‚’è¨ˆç®—"""
    embeddings_np = embeddings.cpu().numpy()
    
    print(f"\n{name}:")
    print(f"  å½¢çŠ¶: {embeddings_np.shape}")
    print(f"  å¹³å‡: {embeddings_np.mean():.6f}")
    print(f"  æ¨™æº–åå·®: {embeddings_np.std():.6f}")
    print(f"  æœ€å°å€¤: {embeddings_np.min():.6f}")
    print(f"  æœ€å¤§å€¤: {embeddings_np.max():.6f}")
    
    # ãƒãƒ«ãƒ è¨ˆç®—
    norms = np.linalg.norm(embeddings_np, axis=1)
    print(f"  L2ãƒãƒ«ãƒ  - å¹³å‡: {norms.mean():.6f}, æ¨™æº–åå·®: {norms.std():.6f}")
    
    # è‡ªå·±é¡ä¼¼åº¦ï¼ˆåŒã˜ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å†…ï¼‰
    from sklearn.metrics.pairwise import cosine_similarity
    sim_matrix = cosine_similarity(embeddings_np)
    # å¯¾è§’ç·šã‚’é™¤å¤–
    sim_values = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]
    print(f"  è‡ªå·±é¡ä¼¼åº¦ - å¹³å‡: {sim_values.mean():.6f}, æ¨™æº–åå·®: {sim_values.std():.6f}")
    
    return embeddings_np, sim_matrix

print("\n" + "=" * 80)
print("ğŸ“Š ColVBERT åŸ‹ã‚è¾¼ã¿çµ±è¨ˆ")
print("=" * 80)
colbert_text_np, colbert_text_sim = analyze_embeddings(colbert_text_emb, "ColVBERT ãƒ†ã‚­ã‚¹ãƒˆ")
colbert_img_np, colbert_img_sim = analyze_embeddings(colbert_img_emb, "ColVBERT ç”»åƒ")

print("\n" + "=" * 80)
print("ğŸ“Š ColModernVBERT åŸ‹ã‚è¾¼ã¿çµ±è¨ˆ")
print("=" * 80)
modern_text_np, modern_text_sim = analyze_embeddings(modern_text_emb, "ColModernVBERT ãƒ†ã‚­ã‚¹ãƒˆ")
modern_img_np, modern_img_sim = analyze_embeddings(modern_img_emb, "ColModernVBERT ç”»åƒ")

# ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«é¡ä¼¼åº¦
print("\n" + "=" * 80)
print("ğŸ“Š ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«é¡ä¼¼åº¦ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ vs ç”»åƒï¼‰")
print("=" * 80)

from sklearn.metrics.pairwise import cosine_similarity

colbert_cross_sim = cosine_similarity(colbert_text_np, colbert_img_np)
modern_cross_sim = cosine_similarity(modern_text_np, modern_img_np)

print(f"\nColVBERT ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«é¡ä¼¼åº¦:")
print(f"  å¹³å‡: {colbert_cross_sim.mean():.6f}")
print(f"  æ¨™æº–åå·®: {colbert_cross_sim.std():.6f}")
print(f"  å¯¾è§’è¦ç´ ï¼ˆå¯¾å¿œãƒšã‚¢ï¼‰å¹³å‡: {np.diag(colbert_cross_sim).mean():.6f}")

print(f"\nColModernVBERT ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«é¡ä¼¼åº¦:")
print(f"  å¹³å‡: {modern_cross_sim.mean():.6f}")
print(f"  æ¨™æº–åå·®: {modern_cross_sim.std():.6f}")
print(f"  å¯¾è§’è¦ç´ ï¼ˆå¯¾å¿œãƒšã‚¢ï¼‰å¹³å‡: {np.diag(modern_cross_sim).mean():.6f}")

# å¯è¦–åŒ–
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# ColVBERT
im1 = axes[0, 0].imshow(colbert_text_sim, cmap='viridis', vmin=0, vmax=1)
axes[0, 0].set_title('ColVBERT Text Self-Similarity')
plt.colorbar(im1, ax=axes[0, 0])

im2 = axes[0, 1].imshow(colbert_img_sim, cmap='viridis', vmin=0, vmax=1)
axes[0, 1].set_title('ColVBERT Image Self-Similarity')
plt.colorbar(im2, ax=axes[0, 1])

im3 = axes[0, 2].imshow(colbert_cross_sim, cmap='viridis', vmin=0, vmax=1)
axes[0, 2].set_title('ColVBERT Cross-Modal Similarity')
plt.colorbar(im3, ax=axes[0, 2])

# ColModernVBERT
im4 = axes[1, 0].imshow(modern_text_sim, cmap='viridis', vmin=0, vmax=1)
axes[1, 0].set_title('ColModernVBERT Text Self-Similarity')
plt.colorbar(im4, ax=axes[1, 0])

im5 = axes[1, 1].imshow(modern_img_sim, cmap='viridis', vmin=0, vmax=1)
axes[1, 1].set_title('ColModernVBERT Image Self-Similarity')
plt.colorbar(im5, ax=axes[1, 1])

im6 = axes[1, 2].imshow(modern_cross_sim, cmap='viridis', vmin=0, vmax=1)
axes[1, 2].set_title('ColModernVBERT Cross-Modal Similarity')
plt.colorbar(im6, ax=axes[1, 2])

plt.tight_layout()
output_path = images_dir / "embedding_similarity_analysis.png"
plt.savefig(output_path, dpi=150, bbox_inches='tight')
print(f"\nâœ… å¯è¦–åŒ–ã‚’ {output_path} ã«ä¿å­˜")

# åˆ†å¸ƒã®æ¯”è¼ƒ
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

axes[0, 0].hist(colbert_text_np.flatten(), bins=50, alpha=0.7, label='ColVBERT Text')
axes[0, 0].set_title('ColVBERT Text Embedding Distribution')
axes[0, 0].set_xlabel('Value')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].legend()

axes[0, 1].hist(colbert_img_np.flatten(), bins=50, alpha=0.7, label='ColVBERT Image')
axes[0, 1].set_title('ColVBERT Image Embedding Distribution')
axes[0, 1].set_xlabel('Value')
axes[0, 1].set_ylabel('Frequency')
axes[0, 1].legend()

axes[1, 0].hist(modern_text_np.flatten(), bins=50, alpha=0.7, label='ColModernVBERT Text', color='orange')
axes[1, 0].set_title('ColModernVBERT Text Embedding Distribution')
axes[1, 0].set_xlabel('Value')
axes[1, 0].set_ylabel('Frequency')
axes[1, 0].legend()

axes[1, 1].hist(modern_img_np.flatten(), bins=50, alpha=0.7, label='ColModernVBERT Image', color='orange')
axes[1, 1].set_title('ColModernVBERT Image Embedding Distribution')
axes[1, 1].set_xlabel('Value')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].legend()

plt.tight_layout()
dist_output_path = images_dir / "embedding_distribution_analysis.png"
plt.savefig(dist_output_path, dpi=150, bbox_inches='tight')
print(f"âœ… åˆ†å¸ƒã‚°ãƒ©ãƒ•ã‚’ {dist_output_path} ã«ä¿å­˜")

print("\n" + "=" * 80)
print("âœ… åŸ‹ã‚è¾¼ã¿åˆ†æå®Œäº†")
print("=" * 80)
