#!/usr/bin/env python3
"""
Immediate GPU Activation Script
GPUã‚’å³åº§ã«ç¨¼åƒã•ã›ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import torch
import torch.nn.functional as F
import numpy as np
import time
import threading
from datetime import datetime

def activate_gpu_immediately():
    """GPUã‚’å³åº§ã«ç¨¼åƒã•ã›ã‚‹"""
    if not torch.cuda.is_available():
        print("âŒ CUDA not available")
        return False
    
    device = torch.device('cuda')
    print(f"ğŸš€ Activating GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # GPUç¨¼åƒç¢ºèªç”¨ã®é‡ã„å‡¦ç†ã‚’å®Ÿè¡Œ
    print("\nğŸ”¥ Starting intensive GPU workload...")
    
    # å¤§ããªè¡Œåˆ—ã‚’ä½œæˆã—ã¦GPUä¸Šã§è¨ˆç®—
    start_time = time.time()
    
    # ãƒ¡ãƒ¢ãƒªã‚’å¤§é‡ã«ä½¿ç”¨ã™ã‚‹å‡¦ç†
    large_tensors = []
    for i in range(10):
        size = 4000 + i * 500  # ã‚µã‚¤ã‚ºã‚’å¾ã€…ã«å¢—ã‚„ã™
        print(f"   Creating {size}x{size} tensor on GPU...")
        
        tensor = torch.randn(size, size, device=device, dtype=torch.float32)
        
        # é›†ç´„çš„ãªè¨ˆç®—
        for _ in range(5):
            tensor = torch.mm(tensor, tensor.t())
            tensor = F.relu(tensor)
            tensor = tensor / (tensor.norm() + 1e-8)
        
        large_tensors.append(tensor)
        
        # GPUä½¿ç”¨çŠ¶æ³è¡¨ç¤º
        memory_used = torch.cuda.memory_allocated() / 1e9
        print(f"      GPU Memory used: {memory_used:.2f} GB")
        
        # çŸ­æ™‚é–“å¾…æ©Ÿã—ã¦GPUä½¿ç”¨ç‡ã‚’ç¶­æŒ
        time.sleep(0.1)
    
    # ã•ã‚‰ã«é›†ç´„çš„ãªå‡¦ç†
    print("\nğŸ§® Running matrix multiplication chain...")
    result = large_tensors[0]
    
    for i, tensor in enumerate(large_tensors[1:], 1):
        print(f"   Chain operation {i}...")
        # å¤§ããªè¡Œåˆ—ã®ä¹—ç®—
        result = torch.mm(result[:2000, :2000], tensor[:2000, :2000])
        result = F.tanh(result)
        
        # GPUåŒæœŸ
        torch.cuda.synchronize()
        
        memory_used = torch.cuda.memory_allocated() / 1e9
        print(f"      Memory: {memory_used:.2f} GB")
    
    elapsed = time.time() - start_time
    
    print(f"\nâœ… GPU activation completed in {elapsed:.2f}s")
    print(f"ğŸ¯ Peak GPU memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB")
    
    return True

def continuous_gpu_load():
    """ç¶™ç¶šçš„ãªGPUè² è·ã‚’ç”Ÿæˆ"""
    device = torch.device('cuda')
    
    print("\nğŸ”„ Starting continuous GPU load...")
    
    # ç¶™ç¶šçš„ãªå‡¦ç†ç”¨ã®ãƒ¢ãƒ‡ãƒ«
    model = torch.nn.Sequential(
        torch.nn.Linear(1024, 2048),
        torch.nn.ReLU(),
        torch.nn.Linear(2048, 4096),
        torch.nn.ReLU(),
        torch.nn.Linear(4096, 2048),
        torch.nn.ReLU(),
        torch.nn.Linear(2048, 1024),
    ).to(device)
    
    batch_count = 0
    
    try:
        while True:
            # ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒãƒç”Ÿæˆ
            batch_size = np.random.randint(256, 1024)
            x = torch.randn(batch_size, 1024, device=device)
            
            # å‰å‘ãè¨ˆç®—
            for _ in range(10):  # è¤‡æ•°å›å®Ÿè¡Œã§è² è·å¢—å¤§
                y = model(x)
                x = F.normalize(y, p=2, dim=1)
            
            batch_count += 1
            
            # é€²æ—è¡¨ç¤º
            if batch_count % 50 == 0:
                memory_used = torch.cuda.memory_allocated() / 1e9
                print(f"   Processed {batch_count} batches, Memory: {memory_used:.2f} GB")
            
            # çŸ­æ™‚é–“å¾…æ©Ÿï¼ˆå®Œå…¨ã«é€£ç¶šçš„ã§ã¯ãªãã€åˆ¶å¾¡å¯èƒ½ã«ï¼‰
            time.sleep(0.01)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ Continuous load stopped by user")

def monitor_gpu_status():
    """GPUçŠ¶æ³ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
    import subprocess
    
    print("\nğŸ“Š GPU Status Monitor started")
    
    try:
        while True:
            try:
                result = subprocess.run([
                    'nvidia-smi', 
                    '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu,power.draw',
                    '--format=csv,noheader,nounits'
                ], capture_output=True, text=True, timeout=5)
                
                if result.returncode == 0:
                    values = result.stdout.strip().split(', ')
                    if len(values) >= 4:
                        gpu_util = values[0]
                        mem_used = values[1]
                        mem_total = values[2]
                        temp = values[3]
                        power = values[4] if len(values) > 4 else "N/A"
                        
                        timestamp = datetime.now().strftime('%H:%M:%S')
                        print(f"ğŸ¯ [{timestamp}] GPU: {gpu_util}%, Memory: {mem_used}/{mem_total}MB, Temp: {temp}Â°C, Power: {power}W")
                        
                        # GPUä½¿ç”¨ç‡ãŒé«˜ã„å ´åˆã«é€šçŸ¥
                        if int(gpu_util) > 80:
                            print("ğŸ”¥ HIGH GPU UTILIZATION DETECTED!")
                
                time.sleep(5)  # 5ç§’é–“éš”
                
            except Exception as e:
                print(f"Error in monitoring: {e}")
                time.sleep(5)
                
    except KeyboardInterrupt:
        print("\nğŸ›‘ GPU monitoring stopped")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("="*80)
    print("ğŸš€ GPU Immediate Activation Script")
    print("RTX 4060 Ti GPUç¨¼åƒå¼·åˆ¶å®Ÿè¡Œ")
    print("="*80)
    
    if not torch.cuda.is_available():
        print("âŒ CUDA not available. Cannot activate GPU.")
        return
    
    # 1. å³åº§ã«GPUã‚’ç¨¼åƒã•ã›ã‚‹
    if activate_gpu_immediately():
        print("\nâœ… GPU is now active!")
        
        # 2. ç¶™ç¶šçš„ãªè² è·ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é–‹å§‹
        monitor_thread = threading.Thread(target=monitor_gpu_status, daemon=True)
        monitor_thread.start()
        
        print("\nğŸ”„ Starting continuous GPU workload...")
        print("Press Ctrl+C to stop")
        
        # 3. ç¶™ç¶šçš„ãªGPUè² è·
        continuous_gpu_load()
    
    print("\nğŸ‘‹ GPU activation script finished")

if __name__ == "__main__":
    main()