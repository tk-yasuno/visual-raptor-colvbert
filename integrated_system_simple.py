"""
Visual RAPTOR ColBERT çµ±åˆã‚·ã‚¹ãƒ†ãƒ  - ä¿®æ­£ç‰ˆ

RAPTOR + ColVBERT + Visual Documentå‡¦ç†ã®çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import sys
import json
import time
import numpy as np
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path
import pandas as pd
from datetime import datetime

# å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from visual_raptor_colbert import VisualRAPTORColBERT, VisualDocument
from jina_vdr_benchmark import JinaVDRBenchmark, VDRQuery, VDRDocument
from enhanced_visual_processing import EnhancedVisualProcessor, ProcessingResult
from disaster_dataset_generator import DisasterDocumentGenerator

# LangChainé–¢é€£
from langchain_core.documents import Document


class DisasterVDREvaluator:
    """ç½å®³VDRè©•ä¾¡å™¨"""
    
    def __init__(self):
        self.metrics = {}
    
    def calculate_precision_recall(
        self,
        retrieved_docs: List[str],
        relevant_docs: List[str]
    ) -> Tuple[float, float, float]:
        """Precision, Recall, F1ã‚’è¨ˆç®—"""
        if not retrieved_docs:
            return 0.0, 0.0, 0.0
        
        retrieved_set = set(retrieved_docs)
        relevant_set = set(relevant_docs)
        
        true_positives = len(retrieved_set & relevant_set)
        
        precision = true_positives / len(retrieved_set) if retrieved_set else 0.0
        recall = true_positives / len(relevant_set) if relevant_set else 0.0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        return precision, recall, f1
    
    def calculate_ndcg(
        self,
        retrieved_docs: List[Tuple[str, float]],
        relevance_judgments: Dict[str, int],
        k: int = 10
    ) -> float:
        """NDCG@kã‚’è¨ˆç®—"""
        if not retrieved_docs:
            return 0.0
        
        # DCGè¨ˆç®—
        dcg = 0.0
        for i, (doc_id, score) in enumerate(retrieved_docs[:k]):
            rel = relevance_judgments.get(doc_id, 0)
            dcg += rel / np.log2(i + 2)
        
        # IDCGè¨ˆç®—
        ideal_rels = sorted(relevance_judgments.values(), reverse=True)[:k]
        idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal_rels))
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def evaluate_system(
        self,
        system_results: Dict[str, List[Tuple[str, float]]],
        benchmark: JinaVDRBenchmark
    ) -> Dict[str, float]:
        """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã‚’è©•ä¾¡"""
        all_precision = []
        all_recall = []
        all_f1 = []
        all_ndcg = []
        
        # é–¢é€£æ€§åˆ¤å®šã‚’è¾æ›¸ã«å¤‰æ›
        relevance_dict = {}
        for judgment in benchmark.relevance_judgments:
            if judgment.query_id not in relevance_dict:
                relevance_dict[judgment.query_id] = {}
            relevance_dict[judgment.query_id][judgment.doc_id] = judgment.relevance
        
        # å„ã‚¯ã‚¨ãƒªã«ã¤ã„ã¦è©•ä¾¡
        for query_id, results in system_results.items():
            if query_id in relevance_dict:
                # é–¢é€£æ–‡æ›¸IDã®ãƒªã‚¹ãƒˆ
                relevant_docs = [
                    doc_id for doc_id, rel in relevance_dict[query_id].items()
                    if rel > 0
                ]
                
                # æ¤œç´¢çµæœã®IDã®ãƒªã‚¹ãƒˆ
                retrieved_docs = [doc_id for doc_id, score in results]
                
                # Precision, Recall, F1
                p, r, f1 = self.calculate_precision_recall(retrieved_docs, relevant_docs)
                all_precision.append(p)
                all_recall.append(r)
                all_f1.append(f1)
                
                # NDCG
                ndcg = self.calculate_ndcg(results, relevance_dict[query_id])
                all_ndcg.append(ndcg)
        
        # å¹³å‡å€¤è¨ˆç®—
        metrics = {
            'precision': np.mean(all_precision) if all_precision else 0.0,
            'recall': np.mean(all_recall) if all_recall else 0.0,
            'f1': np.mean(all_f1) if all_f1 else 0.0,
            'ndcg': np.mean(all_ndcg) if all_ndcg else 0.0,
            'num_queries': len(system_results)
        }
        
        return metrics


class IntegratedVisualRAPTORSystem:
    """çµ±åˆVisual RAPTORã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(
        self,
        embeddings_model,
        llm,
        system_config: Dict[str, Any] = None
    ):
        self.embeddings_model = embeddings_model
        self.llm = llm
        self.config = system_config or self._get_default_config()
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.visual_raptor = None
        self.benchmark = None
        self.visual_processor = None
        self.document_generator = None
        self.evaluator = DisasterVDREvaluator()
        
        # çŠ¶æ…‹ç®¡ç†
        self.is_initialized = False
        self.processing_stats = {}
        
        print(f"ğŸš€ Integrated Visual RAPTOR System initialized")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’å–å¾—"""
        return {
            'data_dir': 'data/integrated_system',
            'benchmark_size': 'small',
            'num_queries': 50,
            'num_documents': 500,
            'colbert_config': {
                'text_model': 'intfloat/multilingual-e5-large',
                'vision_model': 'Salesforce/blip2-opt-2.7b',
                'embedding_dim': 768
            },
            'visual_config': {
                'ocr_engines': ['tesseract', 'easyocr'],
                'languages': ['ja', 'en'],
                'confidence_threshold': 0.5
            },
            'raptor_config': {
                'min_clusters': 2,
                'max_clusters': 5,
                'max_depth': 3,
                'chunk_size': 500,
                'chunk_overlap': 100,
                'selection_strategy': 'silhouette'
            }
        }
    
    def initialize_system(self):
        """ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–"""
        print("ğŸ”§ Initializing system components...")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        data_dir = Path(self.config['data_dir'])
        data_dir.mkdir(parents=True, exist_ok=True)
        
        # Visual RAPTORåˆæœŸåŒ–
        print("   Initializing Visual RAPTOR...")
        self.visual_raptor = VisualRAPTORColBERT(
            embeddings_model=self.embeddings_model,
            llm=self.llm,
            colbert_config=self.config['colbert_config'],
            visual_config=self.config['visual_config'],
            **self.config['raptor_config']
        )
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åˆæœŸåŒ–
        print("   Initializing JinaVDR benchmark...")
        self.benchmark = JinaVDRBenchmark(
            data_dir=str(data_dir / 'jina_vdr'),
            language='ja',
            dataset_size=self.config['benchmark_size']
        )
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–
        print("   Initializing visual processor...")
        self.visual_processor = EnhancedVisualProcessor(
            ocr_engines=self.config['visual_config']['ocr_engines'],
            languages=self.config['visual_config']['languages'],
            confidence_threshold=self.config['visual_config']['confidence_threshold']
        )
        
        # æ–‡æ›¸ç”Ÿæˆå™¨åˆæœŸåŒ–
        print("   Initializing document generator...")
        self.document_generator = DisasterDocumentGenerator(
            output_dir=str(data_dir / 'disaster_documents')
        )
        
        self.is_initialized = True
        print("âœ… System initialization completed")
    
    def setup_benchmark_data(self) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if not self.is_initialized:
            self.initialize_system()
        
        print("ğŸ“Š Setting up benchmark data...")
        
        # ã‚¯ã‚¨ãƒªç”Ÿæˆ
        print(f"   Generating {self.config['num_queries']} queries...")
        queries = self.benchmark.generate_disaster_queries(self.config['num_queries'])
        
        # ç½å®³æ–‡æ›¸ç”Ÿæˆ
        print(f"   Generating {self.config['num_documents']} disaster documents...")
        disaster_docs = self.document_generator.generate_dataset(self.config['num_documents'])
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        benchmark_docs = []
        for doc_info in disaster_docs:
            vdr_doc = VDRDocument(
                doc_id=doc_info['doc_id'],
                image_path=doc_info['image_path'],
                text_content="",  # OCRã§å¾Œã§æŠ½å‡º
                category=doc_info['metadata']['document_type'],
                subcategory=doc_info['metadata'].get('area_name', ''),
                metadata=doc_info['metadata']
            )
            benchmark_docs.append(vdr_doc)
        
        self.benchmark.documents = benchmark_docs
        
        # é–¢é€£æ€§åˆ¤å®šç”Ÿæˆ
        print("   Generating relevance judgments...")
        self.benchmark.generate_relevance_judgments()
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.benchmark.save_benchmark_data()
        
        setup_stats = {
            'num_queries': len(queries),
            'num_documents': len(benchmark_docs),
            'num_judgments': len(self.benchmark.relevance_judgments),
            'setup_time': time.time()
        }
        
        print(f"âœ… Benchmark data setup completed")
        print(f"   Queries: {setup_stats['num_queries']}")
        print(f"   Documents: {setup_stats['num_documents']}")
        print(f"   Judgments: {setup_stats['num_judgments']}")
        
        return setup_stats
    
    def save_results(self, results: Dict[str, Any]):
        """çµæœã‚’ä¿å­˜"""
        results_dir = Path(self.config['data_dir']) / 'results'
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # çµæœã‚’JSONã§ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = results_dir / f'evaluation_results_{timestamp}.json'
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ’¾ Results saved to {results_file}")
    
    def run_simple_demo(self) -> Dict[str, Any]:
        """ç°¡å˜ãªãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ"""
        print("="*80)
        print("ğŸš€ Running Simple Visual RAPTOR Demo")
        print("="*80)
        
        try:
            # 1. ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            if not self.is_initialized:
                self.initialize_system()
            
            # 2. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            setup_stats = self.setup_benchmark_data()
            
            # 3. çµæœä¿å­˜
            demo_results = {
                'start_time': datetime.now().isoformat(),
                'config': self.config,
                'setup_stats': setup_stats,
                'end_time': datetime.now().isoformat(),
                'status': 'completed'
            }
            
            self.save_results(demo_results)
            
            print("\n" + "="*80)
            print("âœ… Simple demo completed successfully!")
            print("="*80)
            
            return demo_results
            
        except Exception as e:
            error_results = {
                'start_time': datetime.now().isoformat(),
                'error': str(e),
                'end_time': datetime.now().isoformat(),
                'status': 'failed'
            }
            
            print(f"\nâŒ Demo failed: {e}")
            return error_results


def create_integrated_system(
    embeddings_model,
    llm,
    config: Dict[str, Any] = None
) -> IntegratedVisualRAPTORSystem:
    """çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ"""
    system = IntegratedVisualRAPTORSystem(
        embeddings_model=embeddings_model,
        llm=llm,
        system_config=config
    )
    return system


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    from langchain_ollama import OllamaEmbeddings, ChatOllama
    
    print("="*80)
    print("Visual RAPTOR ColBERT Integration System")
    print("ç½å®³æ–‡æ›¸æ¤œç´¢ãƒ»æ•™è¨“ç¶™æ‰¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ ")
    print("="*80)
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    print("\nğŸ”§ Initializing models...")
    embeddings = OllamaEmbeddings(
        model="mxbai-embed-large",
        base_url="http://localhost:11434"
    )
    llm = ChatOllama(
        model="granite-code:8b",
        temperature=0,
        base_url="http://localhost:11434",
        timeout=600  # 10åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    )
    
    # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ä½œæˆ
    print("\nğŸš€ Creating integrated system...")
    config = {
        'data_dir': 'data/integrated_visual_raptor',
        'benchmark_size': 'small',
        'num_queries': 10,  # ãƒ‡ãƒ¢ç”¨ã«å°‘ãªã‚
        'num_documents': 20  # ãƒ‡ãƒ¢ç”¨ã«å°‘ãªã‚
    }
    
    system = create_integrated_system(
        embeddings_model=embeddings,
        llm=llm,
        config=config
    )
    
    # ç°¡å˜ãªãƒ‡ãƒ¢å®Ÿè¡Œ
    results = system.run_simple_demo()
    
    print(f"\nğŸ“Š Final Results Summary:")
    if 'setup_stats' in results:
        stats = results['setup_stats']
        print(f"   Queries: {stats.get('num_queries', 0)}")
        print(f"   Documents: {stats.get('num_documents', 0)}")
        print(f"   Judgments: {stats.get('num_judgments', 0)}")
    
    print("\nğŸ‰ System demonstration completed!")


if __name__ == "__main__":
    main()