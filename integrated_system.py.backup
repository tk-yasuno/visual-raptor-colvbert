"""
Visual RAPTOR ColBERT çµ±åˆã‚·ã‚¹ãƒ†ãƒ 

RAPTOR + ColVBERT + Visual Documentå‡¦ç†ã®çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
- JinaVDRãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¯¾å¿œ
- éœ‡ç½é–¢é€£æ–‡æ›¸æ¤œç´¢
- å¤šè¨€èªãƒ»ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¤œç´¢
- æ€§èƒ½è©•ä¾¡æ©Ÿèƒ½

Version: 1.0 - Complete Integration
"""

import os
import sys
import json
import time
import numpy as np
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path
import pandas as pd
from datetime import datetime

# å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from visual_raptor_colbert import VisualRAPTORColBERT, VisualDocument
from jina_vdr_benchmark import JinaVDRBenchmark, VDRQuery, VDRDocument
from enhanced_visual_processing import EnhancedVisualProcessor, ProcessingResult
from disaster_dataset_generator import DisasterDocumentGenerator

# LangChainé–¢é€£
from langchain_core.documents import Document


class DisasterVDREvaluator:
    """ç½å®³VDRè©•ä¾¡å™¨"""
    
    def __init__(self):
        self.metrics = {}
    
    def calculate_precision_recall(
        self,
        retrieved_docs: List[str],
        relevant_docs: List[str]
    ) -> Tuple[float, float, float]:
        """Precision, Recall, F1ã‚’è¨ˆç®—"""
        if not retrieved_docs:
            return 0.0, 0.0, 0.0
        
        retrieved_set = set(retrieved_docs)
        relevant_set = set(relevant_docs)
        
        true_positives = len(retrieved_set & relevant_set)
        
        precision = true_positives / len(retrieved_set) if retrieved_set else 0.0
        recall = true_positives / len(relevant_set) if relevant_set else 0.0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        return precision, recall, f1
    
    def calculate_ndcg(
        self,
        retrieved_docs: List[Tuple[str, float]],
        relevance_judgments: Dict[str, int],
        k: int = 10
    ) -> float:
        """NDCG@kã‚’è¨ˆç®—"""
        if not retrieved_docs:
            return 0.0
        
        # DCGè¨ˆç®—
        dcg = 0.0
        for i, (doc_id, score) in enumerate(retrieved_docs[:k]):
            rel = relevance_judgments.get(doc_id, 0)
            dcg += rel / np.log2(i + 2)
        
        # IDCGè¨ˆç®—
        ideal_rels = sorted(relevance_judgments.values(), reverse=True)[:k]
        idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal_rels))
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def evaluate_system(
        self,
        system_results: Dict[str, List[Tuple[str, float]]],
        benchmark: JinaVDRBenchmark
    ) -> Dict[str, float]:
        """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã‚’è©•ä¾¡"""
        all_precision = []
        all_recall = []
        all_f1 = []
        all_ndcg = []
        
        # é–¢é€£æ€§åˆ¤å®šã‚’è¾æ›¸ã«å¤‰æ›
        relevance_dict = {}
        for judgment in benchmark.relevance_judgments:
            if judgment.query_id not in relevance_dict:
                relevance_dict[judgment.query_id] = {}
            relevance_dict[judgment.query_id][judgment.doc_id] = judgment.relevance
        
        # å„ã‚¯ã‚¨ãƒªã«ã¤ã„ã¦è©•ä¾¡
        for query_id, results in system_results.items():
            if query_id in relevance_dict:
                # é–¢é€£æ–‡æ›¸IDã®ãƒªã‚¹ãƒˆ
                relevant_docs = [
                    doc_id for doc_id, rel in relevance_dict[query_id].items()
                    if rel > 0
                ]
                
                # æ¤œç´¢çµæœã®IDã®ãƒªã‚¹ãƒˆ
                retrieved_docs = [doc_id for doc_id, score in results]
                
                # Precision, Recall, F1
                p, r, f1 = self.calculate_precision_recall(retrieved_docs, relevant_docs)
                all_precision.append(p)
                all_recall.append(r)
                all_f1.append(f1)
                
                # NDCG
                ndcg = self.calculate_ndcg(results, relevance_dict[query_id])
                all_ndcg.append(ndcg)
        
        # å¹³å‡å€¤è¨ˆç®—
        metrics = {
            'precision': np.mean(all_precision) if all_precision else 0.0,
            'recall': np.mean(all_recall) if all_recall else 0.0,
            'f1': np.mean(all_f1) if all_f1 else 0.0,
            'ndcg': np.mean(all_ndcg) if all_ndcg else 0.0,
            'num_queries': len(system_results)
        }
        
        return metrics


class IntegratedVisualRAPTORSystem:
    """
    çµ±åˆVisual RAPTORã‚·ã‚¹ãƒ†ãƒ 
    
    æ©Ÿèƒ½:
    - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    - æ–‡æ›¸å‡¦ç†ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    - ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¤œç´¢
    - æ€§èƒ½è©•ä¾¡
    """
    
    def __init__(
        self,
        embeddings_model,
        llm,
        system_config: Dict[str, Any] = None
    ):
        self.embeddings_model = embeddings_model
        self.llm = llm
        
        # ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
        self.config = system_config or self._get_default_config()
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.visual_raptor = None
        self.benchmark = None
        self.visual_processor = None
        self.document_generator = None
        self.evaluator = DisasterVDREvaluator()
        
        # çŠ¶æ…‹ç®¡ç†
        self.is_initialized = False
        self.processing_stats = {}
        
        print(f"ğŸš€ Integrated Visual RAPTOR System initialized")
        print(f"   Configuration: {list(self.config.keys())}")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’å–å¾—"""
        return {
            'data_dir': 'data/integrated_system',
            'benchmark_size': 'small',  # small: 500, medium: 1000, large: 2000+
            'num_queries': 50,
            'num_documents': 500,
            'colbert_config': {
                'text_model': 'intfloat/multilingual-e5-large',
                'vision_model': 'Salesforce/blip2-opt-2.7b',
                'embedding_dim': 768
            },
            'visual_config': {
                'ocr_engines': ['tesseract', 'easyocr'],
                'languages': ['ja', 'en'],
                'confidence_threshold': 0.5
            },
            'raptor_config': {
                'min_clusters': 2,
                'max_clusters': 5,
                'max_depth': 3,
                'chunk_size': 500,
                'chunk_overlap': 100,
                'selection_strategy': 'silhouette'
            }
        }
    
    def initialize_system(self):
        """ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–"""
        print("ğŸ”§ Initializing system components...")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        data_dir = Path(self.config['data_dir'])
        data_dir.mkdir(parents=True, exist_ok=True)
        
        # Visual RAPTORåˆæœŸåŒ–
        print("   Initializing Visual RAPTOR...")
        self.visual_raptor = VisualRAPTORColBERT(
            embeddings_model=self.embeddings_model,
            llm=self.llm,
            colbert_config=self.config['colbert_config'],
            visual_config=self.config['visual_config'],
            **self.config['raptor_config']
        )
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åˆæœŸåŒ–
        print("   Initializing JinaVDR benchmark...")
        self.benchmark = JinaVDRBenchmark(
            data_dir=str(data_dir / 'jina_vdr'),
            language='ja',
            dataset_size=self.config['benchmark_size']
        )
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–
        print("   Initializing visual processor...")
        self.visual_processor = EnhancedVisualProcessor(
            ocr_engines=self.config['visual_config']['ocr_engines'],
            languages=self.config['visual_config']['languages'],
            confidence_threshold=self.config['visual_config']['confidence_threshold']
        )
        
        # æ–‡æ›¸ç”Ÿæˆå™¨åˆæœŸåŒ–
        print("   Initializing document generator...")
        self.document_generator = DisasterDocumentGenerator(
            output_dir=str(data_dir / 'disaster_documents')
        )
        
        self.is_initialized = True
        print("âœ… System initialization completed")
    
    def setup_benchmark_data(self) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if not self.is_initialized:
            self.initialize_system()
        
        print("ğŸ“Š Setting up benchmark data...")
        
        # ã‚¯ã‚¨ãƒªç”Ÿæˆ
        print(f"   Generating {self.config['num_queries']} queries...")
        queries = self.benchmark.generate_disaster_queries(self.config['num_queries'])
        
        # ç½å®³æ–‡æ›¸ç”Ÿæˆ
        print(f"   Generating {self.config['num_documents']} disaster documents...")
        disaster_docs = self.document_generator.generate_dataset(self.config['num_documents'])
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        benchmark_docs = []
        for doc_info in disaster_docs:
            vdr_doc = VDRDocument(
                doc_id=doc_info['doc_id'],
                image_path=doc_info['image_path'],
                text_content="",  # OCRã§å¾Œã§æŠ½å‡º
                category=doc_info['metadata']['document_type'],
                subcategory=doc_info['metadata'].get('area_name', ''),
                metadata=doc_info['metadata']
            )
            benchmark_docs.append(vdr_doc)
        
        self.benchmark.documents = benchmark_docs
        
        # é–¢é€£æ€§åˆ¤å®šç”Ÿæˆ
        print("   Generating relevance judgments...")
        self.benchmark.generate_relevance_judgments()
        
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.benchmark.save_benchmark_data()
        
        setup_stats = {
            'num_queries': len(queries),
            'num_documents': len(benchmark_docs),
            'num_judgments': len(self.benchmark.relevance_judgments),
            'setup_time': time.time()
        }
        
        print(f"âœ… Benchmark data setup completed")
        print(f"   Queries: {setup_stats['num_queries']}")
        print(f"   Documents: {setup_stats['num_documents']}")
        print(f"   Judgments: {setup_stats['num_judgments']}")
        
        return setup_stats
    
    def process_visual_documents(self) -> Dict[str, Any]:
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’å‡¦ç†"""
        if not self.benchmark or not self.benchmark.documents:
            raise ValueError("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print("ğŸ–¼ï¸ Processing visual documents...")
        
        processed_docs = []
        processing_times = []
        
        for i, doc in enumerate(self.benchmark.documents):
            try:
                start_time = time.time()
                
                # OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                result = self.visual_processor.process_document(doc.image_path)
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ›´æ–°
                doc.text_content = result.text_content
                doc.metadata.update({
                    'processing_confidence': result.confidence_scores['overall_confidence'],
                    'layout_elements': len(result.layout_elements),
                    'tables_detected': len(result.tables)
                })
                
                processed_docs.append(doc)
                processing_times.append(time.time() - start_time)
                
                if (i + 1) % 50 == 0:
                    print(f"   Processed {i + 1}/{len(self.benchmark.documents)} documents")
                    
            except Exception as e:
                print(f"   Error processing {doc.doc_id}: {e}")
        
        self.benchmark.documents = processed_docs
        
        processing_stats = {
            'total_processed': len(processed_docs),
            'avg_processing_time': np.mean(processing_times),
            'total_processing_time': sum(processing_times)
        }
        
        self.processing_stats = processing_stats
        
        print(f"âœ… Visual document processing completed")
        print(f"   Processed: {processing_stats['total_processed']} documents")
        print(f"   Avg time: {processing_stats['avg_processing_time']:.2f}s per doc")
        
        return processing_stats
    
    def build_integrated_index(self) -> Dict[str, Any]:
        """çµ±åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        if not self.benchmark or not self.benchmark.documents:
            raise ValueError("æ–‡æ›¸ãŒå‡¦ç†ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print("ğŸ” Building integrated index...")
        
        start_time = time.time()
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’VisualDocumentã«å¤‰æ›
        visual_docs = []
        for doc in self.benchmark.documents:
            visual_doc = VisualDocument(
                image_path=doc.image_path,
                text_content=doc.text_content,
                layout_elements=[],
                metadata=doc.metadata
            )
            visual_docs.append(visual_doc)
        
        # Visual RAPTOR ã«è¨­å®š
        self.visual_raptor.visual_documents = visual_docs
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        visual_index = self.visual_raptor.build_visual_index()
        
        # ãƒ†ã‚­ã‚¹ãƒˆæ–‡æ›¸æº–å‚™
        text_docs = []
        for doc in self.benchmark.documents:
            langchain_doc = Document(
                page_content=doc.text_content,
                metadata={
                    'doc_id': doc.doc_id,
                    'category': doc.category,
                    'subcategory': doc.subcategory,
                    'image_path': doc.image_path
                }
            )
            text_docs.append(langchain_doc)
        
        # çµ±åˆãƒ„ãƒªãƒ¼æ§‹ç¯‰
        tree = self.visual_raptor.build_hybrid_tree(
            text_documents=text_docs,
            visual_documents=visual_docs,
            save_dir=str(Path(self.config['data_dir']) / 'saved_models' / 'integrated')
        )
        
        build_time = time.time() - start_time
        
        index_stats = {
            'visual_documents': len(visual_docs),
            'text_documents': len(text_docs),
            'build_time': build_time,
            'tree_nodes': self.visual_raptor._count_nodes(tree),
            'tree_depth': self.visual_raptor._get_tree_depth(tree)
        }
        
        print(f"âœ… Integrated index built successfully")
        print(f"   Visual docs: {index_stats['visual_documents']}")
        print(f"   Text docs: {index_stats['text_documents']}")
        print(f"   Tree nodes: {index_stats['tree_nodes']}")
        print(f"   Tree depth: {index_stats['tree_depth']}")
        print(f"   Build time: {build_time:.2f}s")
        
        return index_stats
    
    def run_evaluation(self, top_k: int = 10) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        if not self.visual_raptor or not self.benchmark:
            raise ValueError("ã‚·ã‚¹ãƒ†ãƒ ãŒå®Œå…¨ã«åˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print(f"ğŸ“ˆ Running system evaluation (top_k={top_k})...")
        
        # å„ã‚¯ã‚¨ãƒªã§æ¤œç´¢å®Ÿè¡Œ
        system_results = {}
        search_times = []\n        \n        for query in self.benchmark.queries:\n            try:\n                start_time = time.time()\n                \n                # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ¤œç´¢å®Ÿè¡Œ\n                visual_results = self.visual_raptor.search_visual_documents(\n                    query.text,\n                    top_k=top_k\n                )\n                \n                # çµæœã‚’(doc_id, score)ã®ã‚¿ãƒ—ãƒ«ã«å¤‰æ›\n                results = []\n                for visual_doc, score in visual_results:\n                    # visual_docã®metadataã‹ã‚‰doc_idã‚’å–å¾—\n                    doc_id = None\n                    for doc in self.benchmark.documents:\n                        if doc.image_path == visual_doc.image_path:\n                            doc_id = doc.doc_id\n                            break\n                    \n                    if doc_id:\n                        results.append((doc_id, float(score)))\n                \n                system_results[query.query_id] = results\n                search_times.append(time.time() - start_time)\n                \n            except Exception as e:\n                print(f"   Error processing query {query.query_id}: {e}")\n                system_results[query.query_id] = []\n        \n        # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—\n        metrics = self.evaluator.evaluate_system(system_results, self.benchmark)\n        \n        # è¿½åŠ çµ±è¨ˆ\n        metrics.update({\n            'avg_search_time': np.mean(search_times) if search_times else 0.0,\n            'total_search_time': sum(search_times),\n            'queries_processed': len(system_results)\n        })\n        \n        print(f"âœ… Evaluation completed")\n        print(f"   Precision: {metrics['precision']:.4f}")\n        print(f"   Recall: {metrics['recall']:.4f}")\n        print(f"   F1: {metrics['f1']:.4f}")\n        print(f"   NDCG: {metrics['ndcg']:.4f}")\n        print(f"   Avg search time: {metrics['avg_search_time']:.3f}s")\n        \n        return metrics\n    \n    def save_results(self, results: Dict[str, Any]):\n        """çµæœã‚’ä¿å­˜"""\n        results_dir = Path(self.config['data_dir']) / 'results'\n        results_dir.mkdir(parents=True, exist_ok=True)\n        \n        # çµæœã‚’JSONã§ä¿å­˜\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        results_file = results_dir / f'evaluation_results_{timestamp}.json'\n        \n        with open(results_file, 'w', encoding='utf-8') as f:\n            json.dump(results, f, ensure_ascii=False, indent=2, default=str)\n        \n        print(f"ğŸ’¾ Results saved to {results_file}")\n    \n    def run_complete_pipeline(self) -> Dict[str, Any]:\n        """å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""\n        print("="*80)\n        print("ğŸš€ Running Complete Visual RAPTOR Pipeline")\n        print("="*80)\n        \n        pipeline_results = {\n            'start_time': datetime.now().isoformat(),\n            'config': self.config\n        }\n        \n        try:\n            # 1. ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–\n            if not self.is_initialized:\n                self.initialize_system()\n            \n            # 2. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n            setup_stats = self.setup_benchmark_data()\n            pipeline_results['setup_stats'] = setup_stats\n            \n            # 3. ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸å‡¦ç†\n            processing_stats = self.process_visual_documents()\n            pipeline_results['processing_stats'] = processing_stats\n            \n            # 4. çµ±åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰\n            index_stats = self.build_integrated_index()\n            pipeline_results['index_stats'] = index_stats\n            \n            # 5. ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡\n            evaluation_metrics = self.run_evaluation()\n            pipeline_results['evaluation_metrics'] = evaluation_metrics\n            \n            # 6. çµæœä¿å­˜\n            pipeline_results['end_time'] = datetime.now().isoformat()\n            self.save_results(pipeline_results)\n            \n            print("\\n" + "="*80)\n            print("âœ… Complete pipeline execution finished successfully!")\n            print("="*80)\n            \n            return pipeline_results\n            \n        except Exception as e:\n            pipeline_results['error'] = str(e)\n            pipeline_results['end_time'] = datetime.now().isoformat()\n            \n            print(f"\\nâŒ Pipeline execution failed: {e}")\n            return pipeline_results\n\n\ndef create_integrated_system(\n    embeddings_model,\n    llm,\n    config: Dict[str, Any] = None\n) -> IntegratedVisualRAPTORSystem:\n    """çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆ"""\n    system = IntegratedVisualRAPTORSystem(\n        embeddings_model=embeddings_model,\n        llm=llm,\n        system_config=config\n    )\n    return system\n\n\ndef main():\n    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""\n    from langchain_ollama import OllamaEmbeddings, ChatOllama\n    \n    print("="*80)\n    print("Visual RAPTOR ColBERT Integration System")\n    print("ç½å®³æ–‡æ›¸æ¤œç´¢ãƒ»æ•™è¨“ç¶™æ‰¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ ")\n    print("="*80)\n    \n    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\n    print("\\nğŸ”§ Initializing models...")\n    embeddings = OllamaEmbeddings(\n        model="mxbai-embed-large",\n        base_url="http://localhost:11434"\n    )\n    llm = ChatOllama(\n        model="granite-code:8b",\n        temperature=0,\n        base_url="http://localhost:11434",\n        timeout=600  # 10åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n    )\n    \n    # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ä½œæˆ\n    print("\\nğŸš€ Creating integrated system...")\n    config = {\n        'data_dir': 'data/integrated_visual_raptor',\n        'benchmark_size': 'small',\n        'num_queries': 20,  # ãƒ‡ãƒ¢ç”¨ã«å°‘ãªã‚\n        'num_documents': 50  # ãƒ‡ãƒ¢ç”¨ã«å°‘ãªã‚\n    }\n    \n    system = create_integrated_system(\n        embeddings_model=embeddings,\n        llm=llm,\n        config=config\n    )\n    \n    # å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ\n    results = system.run_complete_pipeline()\n    \n    print(f"\\nğŸ“Š Final Results Summary:")\n    if 'evaluation_metrics' in results:\n        metrics = results['evaluation_metrics']\n        print(f"   Precision: {metrics.get('precision', 0):.4f}")\n        print(f"   Recall: {metrics.get('recall', 0):.4f}")\n        print(f"   F1 Score: {metrics.get('f1', 0):.4f}")\n        print(f"   NDCG: {metrics.get('ndcg', 0):.4f}")\n    \n    print("\\nğŸ‰ System demonstration completed!")\n\n\nif __name__ == "__main__":\n    main()