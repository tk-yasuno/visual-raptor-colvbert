"""
ColModernVBERT ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
SigLIPã‚’ä½¿ç”¨ã—ãŸæœ€æ–°ã®ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
"""

import traceback
from pathlib import Path

print("="*80)
print("ðŸš€ ColModernVBERT with SigLIP - Demo")
print("="*80)

try:
    # Step 1: Import modules
    print("\nðŸ“¦ Step 1: Importing modules...")
    from langchain_ollama import OllamaEmbeddings, ChatOllama
    from visual_raptor_colbert import VisualRAPTORColBERT
    from jina_vdr_benchmark import JinaVDRBenchmark
    print("âœ… Imports successful")
    
    # Step 2: Initialize Ollama models
    print("\nðŸ”§ Step 2: Initializing Ollama models...")
    embeddings = OllamaEmbeddings(
        model="mxbai-embed-large",
        base_url="http://localhost:11434"
    )
    llm = ChatOllama(
        model="granite-code:8b",
        temperature=0,
        base_url="http://localhost:11434",
        timeout=600
    )
    print("âœ… Ollama models initialized")
    
    # Step 3: Initialize Visual RAPTOR with ColModernVBERT
    print("\nâœ¨ Step 3: Initializing Visual RAPTOR with ColModernVBERT (SigLIP)...")
    
    colbert_config = {
        'encoder_type': 'modern',  # é‡è¦ï¼šmodernã‚’æŒ‡å®š
        'text_model': 'google/siglip-base-patch16-224',
        'vision_model': 'google/siglip-base-patch16-224',
        'embedding_dim': 768,
        'use_cross_attention': True
    }
    
    visual_raptor = VisualRAPTORColBERT(
        embeddings_model=embeddings,
        llm=llm,
        colbert_config=colbert_config,
        use_modern_vbert=True,  # ColModernVBERTã‚’ä½¿ç”¨
        max_depth=2,
        chunk_size=300
    )
    print("âœ… ColModernVBERT initialized successfully!")
    
    # Step 4: Initialize JinaVDR Benchmark
    print("\nðŸ“Š Step 4: Initializing JinaVDR Benchmark...")
    benchmark = JinaVDRBenchmark(
        data_dir="data/colmodern_vbert_demo",
        language="ja",
        dataset_size="small"
    )
    print("âœ… Benchmark initialized")
    
    # Step 5: Generate sample queries
    print("\nðŸ” Step 5: Generating sample queries...")
    queries = benchmark.generate_disaster_queries(num_queries=5)
    print(f"âœ… Generated {len(queries)} queries")
    for i, q in enumerate(queries[:3], 1):
        print(f"   {i}. {q.text}")
    
    # Step 6: Create sample documents
    print("\nðŸ“„ Step 6: Creating sample documents...")
    documents = benchmark.create_synthetic_documents(num_documents=20)
    print(f"âœ… Created {len(documents)} documents")
    
    # Step 7: Generate relevance judgments
    print("\nðŸ“Š Step 7: Generating relevance judgments...")
    judgments = benchmark.generate_relevance_judgments(num_judgments_per_query=3)
    print(f"âœ… Generated {len(judgments)} relevance judgments")
    
    # Step 8: Save benchmark data
    print("\nðŸ’¾ Step 8: Saving benchmark data...")
    benchmark.save_benchmark_data()
    print("âœ… Benchmark data saved")
    
    # Step 9: Test encoding (optional)
    print("\nðŸ§ª Step 9: Testing ColModernVBERT encoding...")
    from PIL import Image
    import torch
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆ
    test_image = Image.new('RGB', (224, 224), color='white')
    test_texts = ["ç½å®³æ™‚ã®é¿é›£å ´æ‰€", "Emergency evacuation location"]
    test_images = [test_image, test_image]
    
    try:
        # ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        with torch.no_grad():
            embeddings_result = visual_raptor.colbert_encoder.encode_multimodal(
                texts=test_texts,
                images=test_images
            )
        print(f"âœ… Encoding successful! Shape: {embeddings_result.shape}")
        print(f"   Embedding dimension: {embeddings_result.shape[1]}")
    except Exception as e:
        print(f"âš ï¸ Encoding test skipped: {e}")
    
    # Step 10: Show statistics
    print("\nðŸ“ˆ Step 10: System Statistics")
    stats = benchmark.get_benchmark_statistics()
    print(f"   Queries: {stats['num_queries']}")
    print(f"   Documents: {stats['num_documents']}")
    print(f"   Judgments: {stats['num_judgments']}")
    print(f"   Query categories: {list(stats['query_categories'].keys())}")
    print(f"   Document categories: {list(stats['document_categories'].keys())}")
    
    print("\n" + "="*80)
    print("âœ… ColModernVBERT Demo Completed Successfully!")
    print("="*80)
    
    print("\nðŸ“ Key Features:")
    print("   âœ¨ SigLIP-based multimodal encoding")
    print("   ðŸ”„ Cross-attention between text and images")
    print("   ðŸš€ Improved contrastive learning (sigmoid loss)")
    print("   ðŸ’¾ Efficient token representation")
    
    print("\nðŸ“Š Performance Advantages over ColVBERT:")
    print("   â€¢ Better zero-shot transfer")
    print("   â€¢ More efficient training")
    print("   â€¢ Improved multilingual support")
    print("   â€¢ Reduced memory footprint")
    
    print("\nðŸ“‚ Data Location:")
    print(f"   data/colmodern_vbert_demo/")
    print(f"   - queries/queries.json")
    print(f"   - annotations/documents.json")
    print(f"   - images/ (20 synthetic images)")
    
except Exception as e:
    print(f"\nâŒ Error occurred: {e}")
    print("\nDetailed traceback:")
    traceback.print_exc()
    
    print("\nðŸ’¡ Troubleshooting:")
    print("   1. Ensure transformers>=4.35.0: pip install --upgrade transformers")
    print("   2. Check Ollama is running: ollama list")
    print("   3. Verify GPU availability: nvidia-smi")
    print("   4. Check CUDA compatibility")
