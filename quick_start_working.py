#!/usr/bin/env python3
"""
Simple Quick Start - No Dependencies on Broken Files
ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ - ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ä¾å­˜ãªã—
"""

import sys
from pathlib import Path

print("="*80)
print("ğŸš€ Visual RAPTOR ColBERT - Simple Quick Start")
print("ç½å®³æ–‡æ›¸æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆç°¡æ˜“ç‰ˆï¼‰")
print("="*80)

# 1. Ollamaã®ç¢ºèª
print("\nğŸ”§ Step 1: Checking Ollama connection...")
try:
    from langchain_ollama import OllamaEmbeddings, ChatOllama
    
    embeddings = OllamaEmbeddings(
        model="mxbai-embed-large",
        base_url="http://localhost:11434"
    )
    
    # ãƒ†ã‚¹ãƒˆ
    test_embed = embeddings.embed_query("test")
    print(f"   âœ… Ollama embeddings working (dim: {len(test_embed)})")
    
    llm = ChatOllama(
        model="granite-code:8b",
        temperature=0,
        base_url="http://localhost:11434"
    )
    
    # ãƒ†ã‚¹ãƒˆ
    test_response = llm.invoke("ã“ã‚“ã«ã¡ã¯")
    print(f"   âœ… Ollama LLM working")
    
except Exception as e:
    print(f"   âŒ Ollama failed: {e}")
    print("\nPlease ensure:")
    print("  1. Ollama is running: ollama serve")
    print("  2. Models are available:")
    print("     - ollama pull mxbai-embed-large")
    print("     - ollama pull granite-code:8b")
    sys.exit(1)

# 2. PyTorchã¨GPUã®ç¢ºèª
print("\nğŸ”§ Step 2: Checking PyTorch and GPU...")
try:
    import torch
    
    print(f"   PyTorch version: {torch.__version__}")
    print(f"   CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   âœ… GPU ready for acceleration")
    else:
        print(f"   âš ï¸  GPU not available, using CPU")
        
except Exception as e:
    print(f"   âŒ PyTorch check failed: {e}")

# 3. ç°¡å˜ãªãƒ‡ãƒ¢å®Ÿè¡Œ
print("\nğŸš€ Step 3: Running simple disaster document demo...")

# ã‚µãƒ³ãƒ—ãƒ«ç½å®³æ–‡æ›¸
sample_docs = [
    {"id": "doc1", "title": "åœ°éœ‡ç™ºç”Ÿæ™‚ã®é¿é›£æ‰‹é †", "content": "åœ°éœ‡ãŒç™ºç”Ÿã—ãŸã‚‰ã€ã¾ãšèº«ã®å®‰å…¨ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚æºã‚ŒãŒåã¾ã£ãŸã‚‰ã€ç«ã®å§‹æœ«ã‚’ã—ã¦é¿é›£çµŒè·¯ã‚’ç¢ºèªã—ã¾ã™ã€‚"},
    {"id": "doc2", "title": "æ´¥æ³¢è­¦å ±æ™‚ã®å¯¾å¿œ", "content": "æ´¥æ³¢è­¦å ±ãŒç™ºä»¤ã•ã‚ŒãŸã‚‰ã€ç›´ã¡ã«é«˜å°ã¾ãŸã¯3éšå»ºã¦ä»¥ä¸Šã®é ‘ä¸ˆãªå»ºç‰©ã«é¿é›£ã—ã¦ãã ã•ã„ã€‚"},
    {"id": "doc3", "title": "é¿é›£æ‰€ã§ã®ç”Ÿæ´»ã‚¬ã‚¤ãƒ‰", "content": "é¿é›£æ‰€ã§ã¯è­²ã‚Šåˆã„ã®ç²¾ç¥ã§ç”Ÿæ´»ã—ã¦ãã ã•ã„ã€‚é£Ÿäº‹ã¯æ±ºã‚ã‚‰ã‚ŒãŸæ™‚é–“ã«é…å¸ƒã•ã‚Œã¾ã™ã€‚"},
    {"id": "doc4", "title": "ç·Šæ€¥é€£çµ¡å…ˆä¸€è¦§", "content": "æ¶ˆé˜²ç½²: 119ã€è­¦å¯Ÿç½²: 110ã€å¸‚å½¹æ‰€ç½å®³å¯¾ç­–æœ¬éƒ¨: 045-123-4567"},
    {"id": "doc5", "title": "å‚™è“„å“ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ", "content": "éå¸¸é£Ÿï¼ˆ3æ—¥åˆ†ï¼‰ã€é£²æ–™æ°´ï¼ˆ1äºº1æ—¥3ãƒªãƒƒãƒˆãƒ«Ã—3æ—¥åˆ†ï¼‰ã€æ‡ä¸­é›»ç¯ã€ãƒ©ã‚¸ã‚ªã€å¿œæ€¥åŒ»è–¬å“"}
]

# æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆ
print("   Creating document embeddings...")
doc_texts = [f"{doc['title']}\n{doc['content']}" for doc in sample_docs]

try:
    doc_embeddings = []
    for i, text in enumerate(doc_texts):
        emb = embeddings.embed_query(text)
        doc_embeddings.append(emb)
        if (i + 1) % 2 == 0:
            print(f"      Embedded {i+1}/{len(doc_texts)} documents")
    
    print(f"   âœ… Created {len(doc_embeddings)} document embeddings")
    
except Exception as e:
    print(f"   âŒ Embedding failed: {e}")
    sys.exit(1)

# ã‚¯ã‚¨ãƒªã§æ¤œç´¢
print("\nğŸ” Step 4: Testing document search...")

test_queries = [
    "åœ°éœ‡ãŒèµ·ããŸã¨ãã¯ã©ã†ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ",
    "æ´¥æ³¢è­¦å ±ãŒå‡ºãŸã¨ãã®å¯¾å¿œã¯ï¼Ÿ",
    "ç·Šæ€¥æ™‚ã®é€£çµ¡å…ˆã‚’æ•™ãˆã¦"
]

import numpy as np

for query_text in test_queries:
    print(f"\n   Query: {query_text}")
    
    # ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿
    query_emb = embeddings.embed_query(query_text)
    
    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
    query_vec = np.array(query_emb)
    doc_vecs = np.array(doc_embeddings)
    
    similarities = np.dot(doc_vecs, query_vec) / (
        np.linalg.norm(doc_vecs, axis=1) * np.linalg.norm(query_vec)
    )
    
    # ä¸Šä½3ä»¶å–å¾—
    top_indices = np.argsort(similarities)[::-1][:3]
    
    print("   Top results:")
    for rank, idx in enumerate(top_indices, 1):
        doc = sample_docs[idx]
        score = similarities[idx]
        print(f"      {rank}. {doc['title']} (score: {score:.4f})")

# 5. LLMã§è¦ç´„ç”Ÿæˆ
print("\nğŸ¤– Step 5: Testing LLM summarization...")

top_doc = sample_docs[top_indices[0]]
prompt = f"""
æ¬¡ã®ç½å®³é–¢é€£æ–‡æ›¸ã‚’å‚è€ƒã«ã€è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è³ªå•: {test_queries[0]}

æ–‡æ›¸: {top_doc['title']}
{top_doc['content']}

å›ç­”:"""

try:
    response = llm.invoke(prompt)
    print(f"   LLM Response: {response.content[:200]}...")
    print(f"   âœ… LLM summarization working")
    
except Exception as e:
    print(f"   âŒ LLM failed: {e}")

# ã¾ã¨ã‚
print("\n" + "="*80)
print("âœ… Quick Start Completed Successfully!")
print("="*80)
print("\nğŸ“Š System Status:")
print("   âœ… Ollama embeddings: Working")
print("   âœ… Ollama LLM: Working")
print("   âœ… Document search: Working")
print("   âœ… Summarization: Working")

if torch.cuda.is_available():
    print(f"   âœ… GPU ({torch.cuda.get_device_name(0)}): Available")
else:
    print("   âš ï¸  GPU: Not available (using CPU)")

print("\nğŸ‰ Your Visual RAPTOR ColBERT system is ready!")
print("\nNext steps:")
print("  - Run 'python gpu_utilization_test.py' to test GPU acceleration")
print("  - Check 'data/' directory for results")
print("  - Explore the full system with more documents")

print("\n" + "="*80)
