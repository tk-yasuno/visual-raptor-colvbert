"""
Visual RAPTOR ColBERT - å®Ÿéš›ã®PDFç”»åƒï¼ˆ131æšï¼‰ã«å¯¾ã™ã‚‹å®Ÿè¡Œ
================================================

å‡¦ç†ã•ã‚ŒãŸPDFç”»åƒã«å¯¾ã—ã¦Visual RAPTORã‚’å®Ÿè¡Œã—ã€
éšå±¤çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import os
import sys
import time
import json
from pathlib import Path
from PIL import Image
from datetime import datetime
from typing import List, Tuple, Dict
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # GUIä¸è¦ã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False

# visual_raptor_colbert.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from visual_raptor_colbert import (
    VisualRAPTORColBERT,
    ColModernVBERTEncoder,
    ColVBERTEncoder
)

def load_pdf_images(images_dir: str):
    """PDFç”»åƒã‚’èª­ã¿è¾¼ã‚€"""
    images_path = Path(images_dir)
    
    if not images_path.exists():
        raise FileNotFoundError(f"ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {images_dir}")
    
    # PNGç”»åƒã‚’å–å¾—
    image_files = sorted(images_path.glob("*.png"))
    
    if len(image_files) == 0:
        raise FileNotFoundError(f"ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {images_dir}")
    
    print(f"\nğŸ“ ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {images_dir}")
    print(f"ğŸ“Š æ¤œå‡ºã•ã‚ŒãŸç”»åƒæ•°: {len(image_files)}")
    
    # ç”»åƒã‚’èª­ã¿è¾¼ã¿
    images = []
    metadata = []
    
    print("\nğŸ–¼ï¸ ç”»åƒèª­ã¿è¾¼ã¿ä¸­...")
    for i, img_path in enumerate(image_files, 1):
        try:
            image = Image.open(img_path).convert('RGB')
            images.append(image)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ï¼‰
            filename = img_path.stem
            if "æ±æ—¥æœ¬å¤§éœ‡ç½ã®æ•™è¨“é›†" in filename:
                pdf_name = "æ±æ—¥æœ¬å¤§éœ‡ç½ã®æ•™è¨“é›†"
            elif "ä»¤å’Œ6å¹´åº¦ã®ç½å®³ã‚’ä¸­å¿ƒã¨ã—ãŸäº‹ä¾‹é›†" in filename:
                pdf_name = "ä»¤å’Œ6å¹´åº¦ã®ç½å®³äº‹ä¾‹é›†"
            else:
                pdf_name = "ä¸æ˜"
            
            # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º
            page_num = filename.split("_page")[-1]
            
            metadata.append({
                'pdf_name': pdf_name,
                'page_number': page_num,
                'filename': img_path.name,
                'path': str(img_path)
            })
            
            if i % 20 == 0:
                print(f"  é€²æ—: {i}/{len(image_files)}")
                
        except Exception as e:
            print(f"  âš ï¸ ã‚¨ãƒ©ãƒ¼ ({img_path.name}): {e}")
            continue
    
    print(f"âœ… {len(images)}æšã®ç”»åƒã‚’èª­ã¿è¾¼ã¿å®Œäº†\n")
    return images, metadata

def plot_siglip_metrics(all_results: Dict, output_dir: Path):
    """SigLIPè©•ä¾¡æŒ‡æ¨™ã‚’ã‚°ãƒ©ãƒ•åŒ–ã—ã¦ä¿å­˜"""
    
    queries = [q['query'] for q in all_results['queries']]
    
    # 6ã¤ã®æŒ‡æ¨™ã‚’æŠ½å‡º
    variances = [q['siglip_metrics']['variance'] for q in all_results['queries']]
    entropies = [q['siglip_metrics']['normalized_entropy'] for q in all_results['queries']]
    confidences = [q['siglip_metrics']['confidence'] for q in all_results['queries']]
    dominances = [q['siglip_metrics']['relative_dominance'] for q in all_results['queries']]
    qualities = [q['siglip_metrics']['ranking_quality'] for q in all_results['queries']]
    decays = [q['siglip_metrics']['score_decay_rate'] for q in all_results['queries']]
    
    # å›³ã®ã‚µã‚¤ã‚ºã¨é…ç½®
    fig, axes = plt.subplots(3, 2, figsize=(16, 12))
    fig.suptitle('SigLIPè©•ä¾¡æŒ‡æ¨™ - ã‚¯ã‚¨ãƒªåˆ¥æ¯”è¼ƒ', fontsize=16, fontweight='bold')
    
    # ã‚¯ã‚¨ãƒªç•ªå·ï¼ˆXè»¸ï¼‰
    x_pos = np.arange(len(queries))
    
    # 1. åˆ†æ•£ (Variance)
    ax1 = axes[0, 0]
    bars1 = ax1.bar(x_pos, variances, color='steelblue', alpha=0.7, edgecolor='black')
    ax1.set_title('â‘  åˆ†æ•£ (Variance)\nçµæœã®å¤šæ§˜æ€§', fontsize=12, fontweight='bold')
    ax1.set_xlabel('ã‚¯ã‚¨ãƒªç•ªå·', fontsize=10)
    ax1.set_ylabel('åˆ†æ•£å€¤', fontsize=10)
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels([f'Q{i+1}' for i in range(len(queries))], rotation=0)
    ax1.grid(axis='y', alpha=0.3)
    ax1.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
    
    # 2. æ­£è¦åŒ–ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (Normalized Entropy)
    ax2 = axes[0, 1]
    bars2 = ax2.bar(x_pos, entropies, color='coral', alpha=0.7, edgecolor='black')
    ax2.set_title('â‘¡ æ­£è¦åŒ–ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (Entropy)\nçµæœã®ä¸ç¢ºå®Ÿæ€§ (0:ç¢ºä¿¡çš„, 1:ä¸ç¢ºå®Ÿ)', fontsize=12, fontweight='bold')
    ax2.set_xlabel('ã‚¯ã‚¨ãƒªç•ªå·', fontsize=10)
    ax2.set_ylabel('ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼', fontsize=10)
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels([f'Q{i+1}' for i in range(len(queries))], rotation=0)
    ax2.set_ylim([0, 1.1])
    ax2.grid(axis='y', alpha=0.3)
    
    # 3. ä¿¡é ¼åº¦ (Confidence)
    ax3 = axes[1, 0]
    bars3 = ax3.bar(x_pos, confidences, color='mediumseagreen', alpha=0.7, edgecolor='black')
    ax3.set_title('â‘¢ ä¿¡é ¼åº¦ (Confidence)\nTop1ã¨Top2ã®å·® (é«˜ã„ã»ã©æ˜ç¢º)', fontsize=12, fontweight='bold')
    ax3.set_xlabel('ã‚¯ã‚¨ãƒªç•ªå·', fontsize=10)
    ax3.set_ylabel('ä¿¡é ¼åº¦', fontsize=10)
    ax3.set_xticks(x_pos)
    ax3.set_xticklabels([f'Q{i+1}' for i in range(len(queries))], rotation=0)
    ax3.grid(axis='y', alpha=0.3)
    
    # 4. ç›¸å¯¾å„ªä½æ€§ (Relative Dominance)
    ax4 = axes[1, 1]
    bars4 = ax4.bar(x_pos, dominances, color='orange', alpha=0.7, edgecolor='black')
    ax4.set_title('â‘£ ç›¸å¯¾å„ªä½æ€§ (Dominance)\nTop1ãŒå¹³å‡ã‚’è¶…ãˆã‚‹åº¦åˆã„', fontsize=12, fontweight='bold')
    ax4.set_xlabel('ã‚¯ã‚¨ãƒªç•ªå·', fontsize=10)
    ax4.set_ylabel('å„ªä½æ€§ (æ¨™æº–åå·®å˜ä½)', fontsize=10)
    ax4.set_xticks(x_pos)
    ax4.set_xticklabels([f'Q{i+1}' for i in range(len(queries))], rotation=0)
    ax4.grid(axis='y', alpha=0.3)
    
    # 5. ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ª (Ranking Quality)
    ax5 = axes[2, 0]
    bars5 = ax5.bar(x_pos, qualities, color='mediumpurple', alpha=0.7, edgecolor='black')
    ax5.set_title('â‘¤ ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ª (Quality)\nDCGé¢¨ã‚¹ã‚³ã‚¢ (é«˜ã„ã»ã©è‰¯å¥½)', fontsize=12, fontweight='bold')
    ax5.set_xlabel('ã‚¯ã‚¨ãƒªç•ªå·', fontsize=10)
    ax5.set_ylabel('å“è³ªã‚¹ã‚³ã‚¢', fontsize=10)
    ax5.set_xticks(x_pos)
    ax5.set_xticklabels([f'Q{i+1}' for i in range(len(queries))], rotation=0)
    ax5.grid(axis='y', alpha=0.3)
    ax5.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)
    
    # 6. ã‚¹ã‚³ã‚¢æ¸›è¡°ç‡ (Score Decay Rate)
    ax6 = axes[2, 1]
    bars6 = ax6.bar(x_pos, decays, color='crimson', alpha=0.7, edgecolor='black')
    ax6.set_title('â‘¥ ã‚¹ã‚³ã‚¢æ¸›è¡°ç‡ (Decay)\nãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®æ»‘ã‚‰ã‹ã• (ä½ã„ã»ã©æ»‘ã‚‰ã‹)', fontsize=12, fontweight='bold')
    ax6.set_xlabel('ã‚¯ã‚¨ãƒªç•ªå·', fontsize=10)
    ax6.set_ylabel('æ¸›è¡°ç‡', fontsize=10)
    ax6.set_xticks(x_pos)
    ax6.set_xticklabels([f'Q{i+1}' for i in range(len(queries))], rotation=0)
    ax6.grid(axis='y', alpha=0.3)
    ax6.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
    
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
    plt.tight_layout(rect=[0, 0, 1, 0.97])
    
    # ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"siglip_metrics_{timestamp}.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"\nğŸ“Š ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
    
    # ã‚¯ã‚¨ãƒªãƒªã‚¹ãƒˆã‚‚ä¿å­˜
    query_legend_file = output_dir / f"query_legend_{timestamp}.txt"
    with open(query_legend_file, 'w', encoding='utf-8') as f:
        f.write("ã‚¯ã‚¨ãƒªå‡¡ä¾‹\n")
        f.write("=" * 50 + "\n")
        for i, query in enumerate(queries, 1):
            f.write(f"Q{i}: {query}\n")
    
    print(f"ğŸ“ ã‚¯ã‚¨ãƒªå‡¡ä¾‹ã‚’ä¿å­˜: {query_legend_file}")

def run_visual_raptor_on_pdfs():
    """å®Ÿéš›ã®PDFç”»åƒã«å¯¾ã—ã¦Visual RAPTORã‚’å®Ÿè¡Œ"""
    
    print("=" * 80)
    print("ğŸ¯ Visual RAPTOR ColBERT - å®Ÿéš›ã®PDFç”»åƒå‡¦ç†")
    print("=" * 80)
    
    # ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    images_dir = "data/processed_pdfs/images"
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ç”»åƒèª­ã¿è¾¼ã¿
    print("\n[ã‚¹ãƒ†ãƒƒãƒ— 1/4] PDFç”»åƒèª­ã¿è¾¼ã¿")
    print("-" * 80)
    images, metadata = load_pdf_images(images_dir)
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: Visual RAPTORåˆæœŸåŒ–
    print("\n[ã‚¹ãƒ†ãƒƒãƒ— 2/4] Visual RAPTORåˆæœŸåŒ–")
    print("-" * 80)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # OllamaåˆæœŸåŒ–ï¼ˆãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã¨LLMï¼‰
    from langchain_ollama import OllamaEmbeddings
    from langchain_ollama.llms import OllamaLLM
    
    print("ğŸ”§ OllamaåˆæœŸåŒ–ä¸­...")
    embeddings_model = OllamaEmbeddings(model="mxbai-embed-large")
    llm = OllamaLLM(model="granite-code:8b")
    print("âœ… OllamaåˆæœŸåŒ–å®Œäº†")
    
    # ColModernVBERT (SigLIP) ã‚’ä½¿ç”¨
    print("ğŸš€ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: ColModernVBERT (SigLIP)")
    
    # PDFå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºç”¨ï¼‰
    pdf_source_dir = "data/disaster_visual_documents"
    
    raptor = VisualRAPTORColBERT(
        embeddings_model=embeddings_model,
        llm=llm,
        use_modern_vbert=True,  # ColModernVBERT (SigLIP)ã‚’ä½¿ç”¨
        pdf_source_dir=pdf_source_dir,  # PDFãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºç”¨
        min_clusters=2,         # æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿æ•°
        max_clusters=10,        # æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿æ•°
        max_depth=3,            # éšå±¤ã®æœ€å¤§æ·±ã•
    )
    
    print("âœ… Visual RAPTORåˆæœŸåŒ–å®Œäº†\n")
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: ç”»åƒã‚’Visual Documentsã¨ã—ã¦èª­ã¿è¾¼ã¿
    print("\n[ã‚¹ãƒ†ãƒƒãƒ— 3/4] Visual Documentsèª­ã¿è¾¼ã¿ & ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
    print("-" * 80)
    
    start_time = time.time()
    
    # ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿
    print(f"ğŸ“ ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­...")
    visual_docs = raptor.load_visual_documents(
        image_directory=images_dir,
        supported_formats=['.png']
    )
    
    print(f"âœ… {len(visual_docs)}æšã®Visual Documentsèª­ã¿è¾¼ã¿å®Œäº†")
    
    # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
    print("\nğŸ”¨ ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­...")
    index_start = time.time()
    raptor.build_visual_index()
    index_time = time.time() - index_start
    
    total_time = time.time() - start_time
    
    print(f"âœ… ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº† ({index_time:.2f}ç§’)")
    print(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
    print(f"ğŸ“Š å¹³å‡å‡¦ç†é€Ÿåº¦: {total_time/len(visual_docs):.3f}ç§’/ãƒšãƒ¼ã‚¸")
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ†ã‚¹ãƒˆæ¤œç´¢
    print("\n[ã‚¹ãƒ†ãƒƒãƒ— 4/4] ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ãƒ†ã‚¹ãƒˆ")
    print("-" * 80)
    
    # ç½å®³é–¢é€£ã®ã‚¯ã‚¨ãƒª
    test_queries = [
        "æ±æ—¥æœ¬å¤§éœ‡ç½ã®æ•™è¨“",
        "é¿é›£æ‰€ã®é‹å–¶æ–¹æ³•",
        "ç½å®³å¯¾å¿œã®èª²é¡Œã¨æ”¹å–„ç‚¹",
        "å¾©æ—§å¾©èˆˆã®å–ã‚Šçµ„ã¿",
        "é˜²ç½å¯¾ç­–ã®é‡è¦æ€§",
        "æ´¥æ³¢ã®è¢«å®³çŠ¶æ³",
        "åœ°éœ‡ç™ºç”Ÿæ™‚ã®å¯¾å¿œ",
        "è¢«ç½è€…æ”¯æ´ã®å®Ÿè·µä¾‹"
    ]
    
    print(f"\nğŸ” ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª: {len(test_queries)}å€‹\n")
    
    # è©•ä¾¡çµæœã‚’ä¿å­˜
    all_results = {
        'timestamp': datetime.now().isoformat(),
        'system': {
            'encoder': 'ColModernVBERT (SigLIP)',
            'embedding_dim': 768,
            'device': device,
            'total_documents': len(visual_docs)
        },
        'queries': []
    }
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ” ã‚¯ã‚¨ãƒª {i}: ã€Œ{query}ã€")
        print('='*70)
        
        # æ¤œç´¢å®Ÿè¡Œ
        search_start = time.time()
        results = raptor.search_visual_documents(
            query=query,
            top_k=5
        )
        search_time = time.time() - search_start
        
        print(f"â±ï¸ æ¤œç´¢æ™‚é–“: {search_time*1000:.2f}ms")
        
        # SigLIPç”¨è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        scores = np.array([score for _, score in results])
        
        # åŸºæœ¬çµ±è¨ˆ
        mean_score = np.mean(scores) if len(scores) > 0 else 0
        max_score = np.max(scores) if len(scores) > 0 else 0
        min_score = np.min(scores) if len(scores) > 0 else 0
        score_range = max_score - min_score
        std_score = np.std(scores) if len(scores) > 0 else 0
        
        # SigLIPç‰¹æœ‰ã®æŒ‡æ¨™
        # 1. ã‚¹ã‚³ã‚¢åˆ†æ•£ (Score Variance) - çµæœã®å¤šæ§˜æ€§
        variance = np.var(scores) if len(scores) > 0 else 0
        
        # 2. æ­£è¦åŒ–ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (Normalized Entropy) - çµæœã®ä¸ç¢ºå®Ÿæ€§
        # ã‚¹ã‚³ã‚¢ã‚’ç¢ºç‡åˆ†å¸ƒã«å¤‰æ›ï¼ˆsoftmaxé¢¨ï¼‰
        if len(scores) > 0 and score_range > 1e-10:
            score_probs = np.exp(scores - np.max(scores))
            score_probs = score_probs / np.sum(score_probs)
            entropy = -np.sum(score_probs * np.log(score_probs + 1e-10))
            normalized_entropy = entropy / np.log(len(scores))  # æ­£è¦åŒ–
        else:
            entropy = 0
            normalized_entropy = 0
        
        # 3. ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ (Confidence Score) - Top1ã¨Top2ã®å·®
        confidence = scores[0] - scores[1] if len(scores) > 1 else 0
        
        # 4. ç›¸å¯¾çš„å„ªä½æ€§ (Relative Dominance) - Top1 vs å¹³å‡
        relative_dominance = (scores[0] - mean_score) / (std_score + 1e-10) if std_score > 0 else 0
        
        # 5. ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ª (Ranking Quality) - DCGé¢¨ã®æŒ‡æ¨™
        # Topçµæœã»ã©é«˜ã‚¹ã‚³ã‚¢ã§ã‚ã‚‹ã¹ã
        ranking_quality = 0
        for idx, score in enumerate(scores):
            ranking_quality += score / np.log2(idx + 2)  # NDCGé¢¨
        
        # 6. ã‚¹ã‚³ã‚¢æ¸›è¡°ç‡ (Score Decay Rate) - ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®ä¸€è²«æ€§
        if len(scores) > 1:
            score_diffs = np.diff(scores)  # éš£æ¥ã‚¹ã‚³ã‚¢å·®
            decay_rate = np.mean(np.abs(score_diffs))
        else:
            decay_rate = 0
        
        print(f"\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆ:")
        print(f"   å¹³å‡ã‚¹ã‚³ã‚¢: {mean_score:.4f}")
        print(f"   æœ€å¤§ã‚¹ã‚³ã‚¢: {max_score:.4f}")
        print(f"   æœ€å°ã‚¹ã‚³ã‚¢: {min_score:.4f}")
        print(f"   ã‚¹ã‚³ã‚¢ç¯„å›²: {score_range:.4f}")
        print(f"   æ¨™æº–åå·®: {std_score:.4f}")
        
        print(f"\nğŸ¯ SigLIPè©•ä¾¡æŒ‡æ¨™:")
        print(f"   åˆ†æ•£ (Variance): {variance:.6f}")
        print(f"   æ­£è¦åŒ–ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ (Entropy): {normalized_entropy:.4f}")
        print(f"   ä¿¡é ¼åº¦ (Confidence): {confidence:.4f}")
        print(f"   ç›¸å¯¾å„ªä½æ€§ (Dominance): {relative_dominance:.4f}")
        print(f"   ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ª (Quality): {ranking_quality:.4f}")
        print(f"   ã‚¹ã‚³ã‚¢æ¸›è¡°ç‡ (Decay): {decay_rate:.6f}")
        
        print(f"\nğŸ“Š Top 5 çµæœ:\n")
        
        query_results = {
            'query': query,
            'search_time_ms': search_time * 1000,
            'basic_statistics': {
                'mean_score': float(mean_score),
                'max_score': float(max_score),
                'min_score': float(min_score),
                'score_range': float(score_range),
                'std_score': float(std_score)
            },
            'siglip_metrics': {
                'variance': float(variance),
                'normalized_entropy': float(normalized_entropy),
                'confidence': float(confidence),
                'relative_dominance': float(relative_dominance),
                'ranking_quality': float(ranking_quality),
                'score_decay_rate': float(decay_rate)
            },
            'top_results': []
        }
        
        for rank, (visual_doc, score) in enumerate(results, 1):
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            filename = Path(visual_doc.image_path).name
            if "201205_EastJapanQuakeLesson" in filename:
                pdf_name = "æ±æ—¥æœ¬å¤§éœ‡ç½ã®æ•™è¨“é›†"
            elif "202505_Reiwa6DisasterExamples" in filename:
                pdf_name = "ä»¤å’Œ6å¹´åº¦ã®ç½å®³äº‹ä¾‹é›†"
            else:
                pdf_name = "ä¸æ˜"
            
            # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’æŠ½å‡º
            page_num = filename.split("_page")[-1].replace(".png", "")
            
            # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼(æœ€åˆã®200æ–‡å­—)
            text_preview = visual_doc.text_content[:200] if visual_doc.text_content else ""
            text_preview = text_preview.replace('\n', ' ').strip()
            
            print(f"  {rank}. [é¡ä¼¼åº¦: {score:.4f}]")
            print(f"     ğŸ“„ PDF: {pdf_name}")
            print(f"     ğŸ“– ãƒšãƒ¼ã‚¸: {page_num}")
            print(f"     ğŸ–¼ï¸ ãƒ•ã‚¡ã‚¤ãƒ«: {filename}")
            print(f"     ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(visual_doc.text_content)}æ–‡å­—")
            
            if text_preview:
                print(f"     ğŸ’¬ å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {text_preview}...")
            
            print()
            
            # çµæœã‚’ä¿å­˜
            query_results['top_results'].append({
                'rank': rank,
                'score': float(score),
                'pdf_name': pdf_name,
                'page_number': page_num,
                'filename': filename,
                'text_length': len(visual_doc.text_content),
                'text_preview': text_preview,
                'full_text': visual_doc.text_content
            })
        
        all_results['queries'].append(query_results)
    
    # å…¨ã‚¯ã‚¨ãƒªã®é›†è¨ˆæŒ‡æ¨™ã‚’è¨ˆç®—
    all_confidences = [q['siglip_metrics']['confidence'] for q in all_results['queries']]
    all_entropies = [q['siglip_metrics']['normalized_entropy'] for q in all_results['queries']]
    all_qualities = [q['siglip_metrics']['ranking_quality'] for q in all_results['queries']]
    all_decays = [q['siglip_metrics']['score_decay_rate'] for q in all_results['queries']]
    
    aggregate_metrics = {
        'average_confidence': float(np.mean(all_confidences)),
        'average_entropy': float(np.mean(all_entropies)),
        'average_ranking_quality': float(np.mean(all_qualities)),
        'average_decay_rate': float(np.mean(all_decays)),
        'confidence_std': float(np.std(all_confidences)),
        'entropy_std': float(np.std(all_entropies))
    }
    
    all_results['aggregate_metrics'] = aggregate_metrics
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_dir = Path("results")
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"visual_raptor_results_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æ¤œç´¢çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
    print(f"   ç·ã‚¯ã‚¨ãƒªæ•°: {len(test_queries)}")
    print(f"   å„ã‚¯ã‚¨ãƒªTop5çµæœ + å®Œå…¨ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’å«ã‚€")
    
    # SigLIPè©•ä¾¡æŒ‡æ¨™ã®ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
    print(f"\nğŸ“Š SigLIPè©•ä¾¡æŒ‡æ¨™ã®ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆä¸­...")
    plot_siglip_metrics(all_results, output_dir)
    
    print(f"\nğŸ“Š å…¨ä½“é›†è¨ˆæŒ‡æ¨™ (SigLIP):")
    print(f"   å¹³å‡ä¿¡é ¼åº¦: {aggregate_metrics['average_confidence']:.4f} Â± {aggregate_metrics['confidence_std']:.4f}")
    print(f"   å¹³å‡ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {aggregate_metrics['average_entropy']:.4f} Â± {aggregate_metrics['entropy_std']:.4f}")
    print(f"   å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ª: {aggregate_metrics['average_ranking_quality']:.4f}")
    print(f"   å¹³å‡ã‚¹ã‚³ã‚¢æ¸›è¡°ç‡: {aggregate_metrics['average_decay_rate']:.6f}")
    
    # æœ€çµ‚ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("ğŸ“Š å®Ÿè¡Œã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    print(f"å‡¦ç†ç”»åƒæ•°: {len(visual_docs)}æš")
    print(f"ç·å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
    print(f"  - ç”»åƒèª­ã¿è¾¼ã¿ & ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰: {index_time:.2f}ç§’")
    print(f"å¹³å‡å‡¦ç†é€Ÿåº¦: {total_time/len(visual_docs):.3f}ç§’/ãƒšãƒ¼ã‚¸")
    print(f"\nãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹:")
    print(f"  - ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(raptor.visual_documents)}")
    print(f"  - åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {raptor.visual_embeddings.shape[1] if raptor.visual_embeddings is not None else '?'}")
    print(f"\nä½¿ç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼: ColModernVBERT (SigLIP)")
    print(f"åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: 768")
    print(f"ãƒ‡ãƒã‚¤ã‚¹: cuda")
    
    print("\n" + "=" * 80)
    print("âœ… Visual RAPTORå‡¦ç†å®Œäº†ï¼")
    print("=" * 80)
    
    return raptor, images, metadata

if __name__ == "__main__":
    try:
        raptor, visual_docs, _ = run_visual_raptor_on_pdfs()
        
        print("\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("  1. æ¤œç´¢çµæœã®ç²¾åº¦ã‚’è©•ä¾¡")
        print("  2. ã‚ˆã‚Šå¤šæ§˜ãªã‚¯ã‚¨ãƒªã§ãƒ†ã‚¹ãƒˆ")
        print("  3. ç‰¹å®šã®PDFãƒšãƒ¼ã‚¸ã‚’è©³ç´°ã«èª¿æŸ»")
        print("  4. OCRãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ã—ã¦ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¤œç´¢ã‚’å¼·åŒ–")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
