#!/usr/bin/env python3
"""
Quick GPU Utilization Test
GPUã®ç¨¼åƒã‚’ç¢ºèªã™ã‚‹ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
"""

import torch
import torch.nn.functional as F
import numpy as np
import time
from datetime import datetime

def gpu_warmup_test():
    """GPUç¨¼åƒç¢ºèªç”¨ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ”¥ GPU Warmup Test")
    print("="*50)
    
    if not torch.cuda.is_available():
        print("âŒ CUDA not available")
        return False
    
    device = torch.device('cuda')
    
    # GPUæƒ…å ±è¡¨ç¤º
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # GPUã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    print("\nğŸš€ Starting GPU warmup...")
    
    # å¤§ããªè¡Œåˆ—ã‚’ä½œæˆã—ã¦GPUä¸Šã§è¨ˆç®—
    sizes = [1000, 2000, 4000, 8000]
    
    for size in sizes:
        print(f"   Matrix size: {size}x{size}")
        
        # ãƒ©ãƒ³ãƒ€ãƒ è¡Œåˆ—ã‚’GPUä¸Šã«ä½œæˆ
        start_time = time.time()
        a = torch.randn(size, size, device=device, dtype=torch.float32)
        b = torch.randn(size, size, device=device, dtype=torch.float32)
        
        # è¡Œåˆ—ç©è¨ˆç®—ï¼ˆGPUé›†ç´„çš„ï¼‰
        c = torch.mm(a, b)
        
        # è¿½åŠ è¨ˆç®—ã§GPUä½¿ç”¨ç‡ã‚’ä¸Šã’ã‚‹
        for _ in range(5):
            c = torch.mm(c, a)
            c = F.relu(c)
            c = c / c.norm()
        
        torch.cuda.synchronize()  # GPUåŒæœŸ
        elapsed = time.time() - start_time
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        memory_used = torch.cuda.memory_allocated() / 1e9
        
        print(f"      Time: {elapsed:.3f}s, GPU Memory: {memory_used:.2f} GB")
    
    print("âœ… GPU warmup completed")
    return True

def gpu_embedding_simulation():
    """åŸ‹ã‚è¾¼ã¿å‡¦ç†ã®GPUã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\nğŸ§  GPU Embedding Simulation")
    print("="*50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å¤§è¦æ¨¡åŸ‹ã‚è¾¼ã¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    vocab_size = 50000
    embedding_dim = 1024
    batch_size = 128
    num_batches = 20
    
    print(f"Vocab size: {vocab_size}")
    print(f"Embedding dim: {embedding_dim}")
    print(f"Batch size: {batch_size}")
    
    # åŸ‹ã‚è¾¼ã¿å±¤ã‚’GPUä¸Šã«ä½œæˆ
    embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim).to(device)
    
    print(f"\nğŸš€ Processing {num_batches} batches...")
    
    total_time = 0
    
    for batch_idx in range(num_batches):
        start_time = time.time()
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ç”Ÿæˆ
        token_ids = torch.randint(0, vocab_size, (batch_size, 512), device=device)
        
        # åŸ‹ã‚è¾¼ã¿è¨ˆç®—
        embeddings = embedding_layer(token_ids)
        
        # è¿½åŠ å‡¦ç†ã§GPUä½¿ç”¨ç‡ã‚’ä¸Šã’ã‚‹
        # Attention-like computation
        query = embeddings
        key = embeddings
        value = embeddings
        
        attention_scores = torch.matmul(query, key.transpose(-2, -1))
        attention_probs = F.softmax(attention_scores, dim=-1)
        attention_output = torch.matmul(attention_probs, value)
        
        # Layer normalization
        layer_norm = torch.nn.LayerNorm(embedding_dim).to(device)
        output = layer_norm(attention_output + embeddings)
        
        # Feed forward network
        ffn = torch.nn.Sequential(
            torch.nn.Linear(embedding_dim, 4 * embedding_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(4 * embedding_dim, embedding_dim)
        ).to(device)
        
        final_output = ffn(output)
        
        torch.cuda.synchronize()
        batch_time = time.time() - start_time
        total_time += batch_time
        
        memory_used = torch.cuda.memory_allocated() / 1e9
        
        print(f"   Batch {batch_idx+1:2d}: {batch_time:.3f}s, Memory: {memory_used:.2f} GB")
    
    print(f"\nâœ… Embedding simulation completed")
    print(f"   Total time: {total_time:.2f}s")
    print(f"   Avg batch time: {total_time/num_batches:.3f}s")
    print(f"   Throughput: {batch_size * num_batches / total_time:.2f} samples/sec")

def gpu_search_simulation():
    """æ¤œç´¢å‡¦ç†ã®GPUã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\nğŸ” GPU Search Simulation")
    print("="*50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # å¤§è¦æ¨¡æ¤œç´¢ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    num_documents = 10000
    embedding_dim = 1024
    num_queries = 100
    top_k = 10
    
    print(f"Documents: {num_documents}")
    print(f"Embedding dim: {embedding_dim}")
    print(f"Queries: {num_queries}")
    
    # æ–‡æ›¸åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
    print("\nğŸ“š Generating document embeddings...")
    doc_embeddings = torch.randn(num_documents, embedding_dim, device=device, dtype=torch.float32)
    doc_embeddings = F.normalize(doc_embeddings, p=2, dim=1)
    
    print(f"âœ… Document embeddings ready: {doc_embeddings.shape}")
    
    # æ¤œç´¢å®Ÿè¡Œ
    print(f"\nğŸš€ Running {num_queries} search queries...")
    
    search_times = []
    
    for query_idx in range(num_queries):
        start_time = time.time()
        
        # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        query_embedding = torch.randn(1, embedding_dim, device=device, dtype=torch.float32)
        query_embedding = F.normalize(query_embedding, p=2, dim=1)
        
        # é¡ä¼¼åº¦è¨ˆç®—ï¼ˆGPUä¸Šã§ä¸¦åˆ—å®Ÿè¡Œï¼‰
        similarities = torch.mm(query_embedding, doc_embeddings.t()).squeeze(0)
        
        # Top-Kå–å¾—
        top_scores, top_indices = torch.topk(similarities, k=top_k)
        
        torch.cuda.synchronize()
        search_time = time.time() - start_time
        search_times.append(search_time)
        
        if (query_idx + 1) % 20 == 0:
            avg_time = np.mean(search_times[-20:])
            memory_used = torch.cuda.memory_allocated() / 1e9
            print(f"   Query {query_idx+1:3d}: {search_time:.4f}s (avg: {avg_time:.4f}s), Memory: {memory_used:.2f} GB")
    
    print(f"\nâœ… Search simulation completed")
    print(f"   Total queries: {num_queries}")
    print(f"   Avg search time: {np.mean(search_times):.4f}s")
    print(f"   Search throughput: {num_queries / np.sum(search_times):.2f} queries/sec")
    print(f"   GPU memory peak: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB")

def monitor_gpu_usage():
    """GPUä½¿ç”¨çŠ¶æ³ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
    print("\nğŸ“Š GPU Usage Monitoring")
    print("="*50)
    
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu', '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        
        if result.returncode == 0:
            gpu_util, mem_used, mem_total, temp = result.stdout.strip().split(', ')
            
            print(f"GPU Utilization: {gpu_util}%")
            print(f"Memory Used: {mem_used} MB / {mem_total} MB ({int(mem_used)/int(mem_total)*100:.1f}%)")
            print(f"Temperature: {temp}Â°C")
            
            if int(gpu_util) > 50:
                print("âœ… GPU is actively utilized!")
            else:
                print("âš ï¸ GPU utilization is low")
        else:
            print("âŒ Failed to get GPU status")
            
    except Exception as e:
        print(f"âŒ Error monitoring GPU: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("="*80)
    print("ğŸš€ GPU Utilization Test Suite")
    print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    if not torch.cuda.is_available():
        print("âŒ CUDA is not available. Cannot run GPU tests.")
        return False
    
    try:
        # 1. GPUæƒ…å ±ã¨ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        gpu_warmup_test()
        
        # 2. åŸ‹ã‚è¾¼ã¿å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        gpu_embedding_simulation()
        
        # 3. æ¤œç´¢å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        gpu_search_simulation()
        
        # 4. GPUä½¿ç”¨çŠ¶æ³ç¢ºèª
        monitor_gpu_usage()
        
        print("\n" + "="*80)
        print("ğŸ‰ GPU Test Suite Completed!")
        print("="*80)
        print("Your RTX 4060 Ti should now be actively utilized.")
        print("Check nvidia-smi to confirm GPU usage.")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ GPU test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()