"""
Quick Start Demo
Visual RAPTOR ColBERT ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆãƒ‡ãƒ¢
"""

import os
import sys
from pathlib import Path

# ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent))

from integrated_system import create_integrated_system
from langchain_ollama import OllamaEmbeddings, ChatOllama


def check_prerequisites():
    """å‰ææ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯"""
    print("ğŸ”§ Checking prerequisites...")
    
    # Ollamaã‚µãƒ¼ãƒãƒ¼ã®ç¢ºèª
    try:
        embeddings = OllamaEmbeddings(model="mxbai-embed-large")
        test_result = embeddings.embed_query("test")
        print("   âœ… Ollama embeddings working")
    except Exception as e:
        print(f"   âŒ Ollama embeddings failed: {e}")
        print("   Please ensure Ollama is running: ollama serve")
        return False
    
    try:
        llm = ChatOllama(model="granite-code:8b")
        test_result = llm.invoke("test")
        print("   âœ… Ollama LLM working")
    except Exception as e:
        print(f"   âŒ Ollama LLM failed: {e}")
        print("   Please ensure granite-code:8b model is available: ollama pull granite-code:8b")
        return False
    
    return True


def run_quick_demo():
    """ã‚¯ã‚¤ãƒƒã‚¯ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ"""
    print("="*80)
    print("ğŸš€ Visual RAPTOR ColBERT Quick Start Demo")
    print("ç½å®³æ–‡æ›¸æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ")
    print("="*80)
    
    # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
    if not check_prerequisites():
        print("\nâŒ Prerequisites not met. Please fix the issues above.")
        return
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    print("\nğŸ“š Initializing models...")
    embeddings = OllamaEmbeddings(
        model="mxbai-embed-large",
        base_url="http://localhost:11434"
    )
    llm = ChatOllama(
        model="granite-code:8b",
        temperature=0,
        base_url="http://localhost:11434",
        timeout=300
    )
    
    # è»½é‡è¨­å®šã§ã‚·ã‚¹ãƒ†ãƒ ä½œæˆ
    print("\nğŸ”§ Creating system with lightweight configuration...")
    config = {
        'data_dir': 'data/quick_demo',
        'benchmark_size': 'small',
        'num_queries': 10,    # å°‘æ•°ã§ãƒ†ã‚¹ãƒˆ
        'num_documents': 20,  # å°‘æ•°ã§ãƒ†ã‚¹ãƒˆ
        'raptor_config': {
            'min_clusters': 2,
            'max_clusters': 3,
            'max_depth': 2,     # æµ…ã‚ã®ãƒ„ãƒªãƒ¼
            'chunk_size': 300,
            'chunk_overlap': 50
        }
    }
    
    system = create_integrated_system(
        embeddings_model=embeddings,
        llm=llm,
        config=config
    )
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    print("\nğŸƒ Running demonstration pipeline...")
    try:
        results = system.run_complete_pipeline()
        
        # çµæœè¡¨ç¤º
        print("\n" + "="*80)
        print("ğŸ“Š Demo Results Summary")
        print("="*80)
        
        if 'setup_stats' in results:
            setup = results['setup_stats']
            print(f"ğŸ“„ Documents created: {setup.get('num_documents', 0)}")
            print(f"ğŸ” Queries generated: {setup.get('num_queries', 0)}")
        
        if 'processing_stats' in results:
            proc = results['processing_stats']
            print(f"âš¡ Processing time: {proc.get('avg_processing_time', 0):.2f}s per doc")
        
        if 'index_stats' in results:
            idx = results['index_stats']
            print(f"ğŸŒ² Tree nodes: {idx.get('tree_nodes', 0)}")
            print(f"ğŸ“Š Tree depth: {idx.get('tree_depth', 0)}")
        
        if 'evaluation_metrics' in results:
            metrics = results['evaluation_metrics']
            print(f"ğŸ¯ Precision: {metrics.get('precision', 0):.4f}")
            print(f"ğŸ“ˆ Recall: {metrics.get('recall', 0):.4f}")
            print(f"â­ F1 Score: {metrics.get('f1', 0):.4f}")
            print(f"ğŸ”¢ NDCG: {metrics.get('ndcg', 0):.4f}")
        
        print("\nğŸ‰ Quick demo completed successfully!")
        print("âœ¨ You can now explore the full system capabilities.")
        
    except Exception as e:
        print(f"\nâŒ Demo failed: {e}")
        print("Please check the error messages above and try again.")


def show_next_steps():
    """æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¡¨ç¤º"""
    print("\n" + "="*80)
    print("ğŸ“š Next Steps")
    print("="*80)
    print("1. ğŸ“– Read the full README.md for detailed documentation")
    print("2. ğŸ”§ Customize the configuration in integrated_system.py")
    print("3. ğŸ“Š Run with larger datasets for better evaluation")
    print("4. ğŸ–¼ï¸ Add your own disaster documents for processing")
    print("5. ğŸš€ Deploy the system for production use")
    
    print("\nğŸ“ Generated files location:")
    print("   - data/quick_demo/")
    print("   - disaster_documents/")
    print("   - results/")


if __name__ == "__main__":
    run_quick_demo()
    show_next_steps()