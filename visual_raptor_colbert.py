"""
Visual RAPTOR with ColBERT Integration
ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã«å¯¾å¿œã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ 

æ±æ—¥æœ¬å¤§éœ‡ç½ã®æ•™è¨“ã‚’ç¶™æ‰¿ã™ã‚‹ãŸã‚ã®è¦–è¦šçš„æ–‡æ›¸æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
- ColVBERT (Transformers base) ã«ã‚ˆã‚‹ç”»åƒãƒ»æ–‡æ›¸çµ±åˆæ¤œç´¢
- JinaVDR ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¯¾å¿œ
- OCRãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”»åƒå‡¦ç†
- è¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ–‡æ›¸ï¼ˆã‚°ãƒ©ãƒ•ã€è¡¨ã€ã‚¹ã‚­ãƒ£ãƒ³ã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼‰ã¸ã®å¯¾å¿œ

Version: 1.0 - Visual Document Edition
"""

import torch
import torch.nn as nn
from transformers import (
    AutoModel, AutoTokenizer, AutoProcessor,
    VisionEncoderDecoderModel, BlipProcessor, BlipForConditionalGeneration
)
from PIL import Image
import pytesseract
import cv2
import fitz  # PyMuPDF for PDF text extraction
import numpy as np
import faiss
import json
import pickle
from typing import List, Dict, Tuple, Optional, Union
from pathlib import Path
import time
from dataclasses import dataclass
from langchain_core.documents import Document

# ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
sys.path.append(str(Path(__file__).parent / "0_base_tsunami-lesson-rag"))
from tsunami_lesson_raptor import TsunamiLessonRAPTOR


@dataclass
class VisualDocument:
    """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’è¡¨ç¾ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    image_path: str
    text_content: str = ""  # OCRã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    layout_elements: List[Dict] = None  # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ ï¼ˆè¡¨ã€ã‚°ãƒ©ãƒ•ãªã©ï¼‰
    metadata: Dict = None
    
    def __post_init__(self):
        if self.layout_elements is None:
            self.layout_elements = []
        if self.metadata is None:
            self.metadata = {}


class ColModernVBERTEncoder(nn.Module):
    """
    ColModernVBERT ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆæœ€æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰
    SigLIPã‚’ä½¿ç”¨ã—ãŸé«˜æ€§èƒ½ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¡¨ç¾å­¦ç¿’
    
    ç‰¹å¾´:
    - SigLIP (Sigmoid Loss for Language-Image Pre-training)
    - æ”¹å–„ã•ã‚ŒãŸã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå­¦ç¿’
    - ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒˆãƒ¼ã‚¯ãƒ³è¡¨ç¾
    - ãƒãƒƒãƒå˜ä½ã®æœ€é©åŒ–
    """
    
    def __init__(
        self,
        text_model_name: str = "google/siglip-base-patch16-224",
        vision_model_name: str = "google/siglip-base-patch16-224",
        embedding_dim: int = 768,
        use_cross_attention: bool = True,
        device: str = None
    ):
        super().__init__()
        
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.use_cross_attention = use_cross_attention
        
        try:
            # SigLIPçµ±åˆãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ãƒ“ã‚¸ãƒ§ãƒ³ï¼‰
            from transformers import AutoModel, AutoProcessor
            self.model = AutoModel.from_pretrained(text_model_name)
            self.processor = AutoProcessor.from_pretrained(vision_model_name)
            
            # SigLIPã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’å–å¾—
            siglip_dim = self.model.config.projection_dim if hasattr(self.model.config, 'projection_dim') else 768
            
            # ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if self.use_cross_attention:
                self.cross_attention = nn.MultiheadAttention(
                    embed_dim=siglip_dim,
                    num_heads=8,
                    dropout=0.1,
                    batch_first=True
                )
            
            # æ¬¡å…ƒèª¿æ•´ãƒ¬ã‚¤ãƒ¤ãƒ¼
            self.projection = nn.Sequential(
                nn.Linear(siglip_dim, embedding_dim),
                nn.LayerNorm(embedding_dim),
                nn.GELU(),
                nn.Dropout(0.1)
            )
            
            # çµ±åˆãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒï¼‰
            self.fusion_layer = nn.Sequential(
                nn.Linear(embedding_dim * 2, embedding_dim * 2),
                nn.LayerNorm(embedding_dim * 2),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(embedding_dim * 2, embedding_dim),
                nn.LayerNorm(embedding_dim)
            )
            
            self.embedding_dim = embedding_dim
            self.to(self.device)
            
            print(f"âœ… ColModernVBERT initialized with SigLIP")
            print(f"   Device: {self.device}")
            print(f"   Embedding dim: {embedding_dim}")
            print(f"   Cross-attention: {use_cross_attention}")
            
        except ImportError:
            raise ImportError(
                "SigLIP requires transformers>=4.35.0. "
                "Please upgrade: pip install --upgrade transformers"
            )
    
    def encode_text(self, texts: List[str]) -> torch.Tensor:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆSigLIPä½¿ç”¨ï¼‰"""
        inputs = self.processor(
            text=texts,
            padding=True,
            truncation=True,
            max_length=64,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.get_text_features(**inputs)
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
            embeddings = self.projection(outputs)
            # L2æ­£è¦åŒ–ã‚’è¿½åŠ 
            embeddings = nn.functional.normalize(embeddings, p=2, dim=1)
        
        return embeddings
    
    def encode_image(self, images: List[Image.Image]) -> torch.Tensor:
        """ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆSigLIPä½¿ç”¨ï¼‰"""
        inputs = self.processor(
            images=images,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.get_image_features(**inputs)
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³
            embeddings = self.projection(outputs)
            # L2æ­£è¦åŒ–ã‚’è¿½åŠ 
            embeddings = nn.functional.normalize(embeddings, p=2, dim=1)
        
        return embeddings
    
    def encode_multimodal(
        self,
        texts: List[str],
        images: List[Image.Image]
    ) -> torch.Tensor:
        """ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®çµ±åˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        text_embeddings = self.encode_text(texts)
        image_embeddings = self.encode_image(images)
        
        # ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹ç›¸äº’ä½œç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.use_cross_attention:
            # ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³
            text_attended, _ = self.cross_attention(
                text_embeddings.unsqueeze(1),
                image_embeddings.unsqueeze(1),
                image_embeddings.unsqueeze(1)
            )
            text_embeddings = text_attended.squeeze(1)
        
        # çµåˆã¨èåˆ
        combined = torch.cat([text_embeddings, image_embeddings], dim=1)
        fused_embeddings = self.fusion_layer(combined)
        
        # æœ€çµ‚çš„ã«L2æ­£è¦åŒ–
        fused_embeddings = nn.functional.normalize(fused_embeddings, p=2, dim=1)
        
        return fused_embeddings
    
    def compute_similarity(
        self,
        text_embeddings: torch.Tensor,
        image_embeddings: torch.Tensor
    ) -> torch.Tensor:
        """
        SigLIPé¢¨ã®ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é¡ä¼¼åº¦è¨ˆç®—
        
        å¾“æ¥ã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå­¦ç¿’ï¼ˆsoftmaxï¼‰ã§ã¯ãªãã€
        sigmoid lossã‚’ä½¿ç”¨ã—ãŸé¡ä¼¼åº¦
        """
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
        text_norm = nn.functional.normalize(text_embeddings, p=2, dim=1)
        image_norm = nn.functional.normalize(image_embeddings, p=2, dim=1)
        
        similarity = torch.matmul(text_norm, image_norm.t())
        
        # SigLIPã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’å¯èƒ½ã«ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ï¼‰
        temperature = 10.0
        similarity = similarity * temperature
        
        return similarity


class ColVBERTEncoder(nn.Module):
    """
    ColVBERT ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆTransformers baseï¼‰
    ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®çµ±åˆè¡¨ç¾ã‚’å­¦ç¿’
    """
    
    def __init__(
        self,
        text_model_name: str = "intfloat/multilingual-e5-large",
        vision_model_name: str = "Salesforce/blip-image-captioning-base",
        embedding_dim: int = 768,
        device: str = None
    ):
        super().__init__()
        
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        self.text_encoder = AutoModel.from_pretrained(text_model_name)
        self.text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)
        
        # ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’å–å¾—
        text_hidden_size = self.text_encoder.config.hidden_size
        
        # ãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        self.vision_processor = BlipProcessor.from_pretrained(vision_model_name)
        self.vision_encoder = BlipForConditionalGeneration.from_pretrained(vision_model_name)
        
        # ãƒ“ã‚¸ãƒ§ãƒ³åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’å–å¾—
        vision_hidden_size = self.vision_encoder.config.vision_config.hidden_size
        
        # ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ“ã‚¸ãƒ§ãƒ³ã®æŠ•å½±å±¤
        self.text_projection = nn.Linear(text_hidden_size, embedding_dim)
        self.vision_projection = nn.Linear(vision_hidden_size, embedding_dim)
        
        # çµ±åˆãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.fusion_layer = nn.Sequential(
            nn.Linear(embedding_dim * 2, embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embedding_dim, embedding_dim)
        )
        
        self.embedding_dim = embedding_dim
        self.to(self.device)
    
    def encode_text(self, texts: List[str]) -> torch.Tensor:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        inputs = self.text_tokenizer(
            texts, 
            padding=True, 
            truncation=True, 
            max_length=512,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.text_encoder(**inputs)
            embeddings = outputs.last_hidden_state.mean(dim=1)
            # æŠ•å½±å±¤ã‚’é€šã—ã¦åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã«å¤‰æ›
            embeddings = self.text_projection(embeddings)
            # L2æ­£è¦åŒ–ã‚’è¿½åŠ 
            embeddings = nn.functional.normalize(embeddings, p=2, dim=1)
        
        return embeddings
    
    def encode_image(self, images: List[Image.Image]) -> torch.Tensor:
        """ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        inputs = self.vision_processor(
            images=images,
            return_tensors="pt"
        ).to(self.device)
        
        with torch.no_grad():
            # BLIPForConditionalGeneration uses vision_model for image features
            outputs = self.vision_encoder.vision_model(inputs['pixel_values'])
            # CLSãƒˆãƒ¼ã‚¯ãƒ³ã§ã¯ãªãå¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨
            image_features = outputs.last_hidden_state.mean(dim=1)
            # æŠ•å½±å±¤ã‚’é€šã—ã¦åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã«å¤‰æ›
            image_features = self.vision_projection(image_features)
            # L2æ­£è¦åŒ–ã‚’è¿½åŠ 
            image_features = nn.functional.normalize(image_features, p=2, dim=1)
        
        return image_features
    
    def encode_multimodal(
        self,
        texts: List[str],
        images: List[Image.Image]
    ) -> torch.Tensor:
        """ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®çµ±åˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°"""
        text_embeddings = self.encode_text(texts)
        image_embeddings = self.encode_image(images)
        
        # æ¬¡å…ƒã‚’åˆã‚ã›ã‚‹
        if image_embeddings.size(1) != text_embeddings.size(1):
            image_embeddings = nn.functional.adaptive_avg_pool1d(
                image_embeddings.unsqueeze(0), 
                text_embeddings.size(1)
            ).squeeze(0)
        
        # çµåˆ
        combined = torch.cat([text_embeddings, image_embeddings], dim=1)
        
        # èåˆ
        fused_embeddings = self.fusion_layer(combined)
        
        # æœ€çµ‚çš„ã«L2æ­£è¦åŒ–
        fused_embeddings = nn.functional.normalize(fused_embeddings, p=2, dim=1)
        
        return fused_embeddings


class VisualDocumentProcessor:
    """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, ocr_config: Dict = None, pdf_dir: str = None):
        self.ocr_config = ocr_config or {
            'lang': 'jpn+eng',
            'config': '--psm 6'
        }
        self.pdf_dir = pdf_dir  # PDFå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.pdf_text_cache = {}  # PDFãƒ†ã‚­ã‚¹ãƒˆã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        
        # PDFãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã‚’äº‹å‰æŠ½å‡º
        if pdf_dir:
            self._extract_pdf_texts(pdf_dir)
    
    def _extract_pdf_texts(self, pdf_dir: str):
        """PDFãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        pdf_path = Path(pdf_dir)
        if not pdf_path.exists():
            return
        
        for pdf_file in pdf_path.glob("*.pdf"):
            try:
                doc = fitz.open(str(pdf_file))
                pdf_name = pdf_file.stem
                
                # å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                for page_num in range(len(doc)):
                    page = doc[page_num]
                    text = page.get_text()
                    
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«å_ãƒšãƒ¼ã‚¸ç•ªå·
                    cache_key = f"{pdf_name}_page{page_num+1:03d}"
                    self.pdf_text_cache[cache_key] = text.strip()
                
                print(f"  âœ… PDFæŠ½å‡º: {pdf_name} ({len(doc)}ãƒšãƒ¼ã‚¸)")
                doc.close()  # ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã«ã‚¯ãƒ­ãƒ¼ã‚º
                
            except Exception as e:
                print(f"  âš ï¸ PDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({pdf_file.name}): {e}")
            except Exception as e:
                print(f"  âš ï¸ PDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼ ({pdf_file.name}): {e}")
    
    def get_text_from_cache(self, image_filename: str) -> str:
        """ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸPDFãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰.pngã‚’é™¤å»ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ä½œæˆ
        cache_key = image_filename.replace('.png', '').replace('.jpg', '').replace('.jpeg', '')
        return self.pdf_text_cache.get(cache_key, "")
    
    def extract_text_from_image(self, image_path: str) -> str:
        """OCRã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                return ""
            
            # å‰å‡¦ç†
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            # ãƒã‚¤ã‚ºé™¤å»
            denoised = cv2.fastNlMeansDenoising(gray)
            # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå‘ä¸Š
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            enhanced = clahe.apply(denoised)
            
            # OCRå®Ÿè¡Œ
            text = pytesseract.image_to_string(
                enhanced,
                lang=self.ocr_config['lang'],
                config=self.ocr_config['config']
            )
            
            return text.strip()
        except Exception as e:
            # OCRã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆTesseractæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®å ´åˆï¼‰
            return ""
    
    def detect_layout_elements(self, image_path: str) -> List[Dict]:
        """ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ ã‚’æ¤œå‡º"""
        try:
            image = cv2.imread(image_path)
            if image is None:
                return []
            
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            elements = []
            
            # è¡¨ã®æ¤œå‡ºï¼ˆå˜ç´”ãªçŸ©å½¢æ¤œå‡ºï¼‰
            contours, _ = cv2.findContours(gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            for contour in contours:
                x, y, w, h = cv2.boundingRect(contour)
                if w > 100 and h > 50:  # æœ€å°ã‚µã‚¤ã‚ºé–¾å€¤
                    elements.append({
                        'type': 'table_or_figure',
                        'bbox': [x, y, w, h],
                        'area': w * h
                    })
            
            return elements
        except Exception as e:
            # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ¤œå‡ºã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
            return []
    
    def process_visual_document(self, image_path: str) -> VisualDocument:
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’å‡¦ç†"""
        # ã¾ãšPDFã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        image_filename = Path(image_path).name
        text_content = self.get_text_from_cache(image_filename)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ãªã„å ´åˆã¯OCRã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not text_content:
            text_content = self.extract_text_from_image(image_path)
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ æ¤œå‡º
        layout_elements = self.detect_layout_elements(image_path)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = {
            'image_size': self._get_image_size(image_path),
            'processing_time': time.time()
        }
        
        return VisualDocument(
            image_path=image_path,
            text_content=text_content,
            layout_elements=layout_elements,
            metadata=metadata
        )
    
    def _get_image_size(self, image_path: str) -> Tuple[int, int]:
        """ç”»åƒã‚µã‚¤ã‚ºã‚’å–å¾—"""
        try:
            with Image.open(image_path) as img:
                return img.size
        except:
            return (0, 0)


class VisualRAPTORColBERT(TsunamiLessonRAPTOR):
    """
    ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸å¯¾å¿œRAPTOR + ColBERT ã‚·ã‚¹ãƒ†ãƒ 
    
    æ©Ÿèƒ½:
    - OCRãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”»åƒå‡¦ç†
    - ColVBERT / ColModernVBERT ã«ã‚ˆã‚‹çµ±åˆæ¤œç´¢
    - JinaVDR ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¯¾å¿œ
    - è¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ–‡æ›¸ã¸ã®å¯¾å¿œ
    """
    
    def __init__(
        self,
        embeddings_model,
        llm,
        colbert_config: Dict = None,
        visual_config: Dict = None,
        use_modern_vbert: bool = False,
        pdf_source_dir: str = None,  # PDFå…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        **kwargs
    ):
        super().__init__(
            embeddings_model=embeddings_model,
            llm=llm,
            **kwargs
        )
        
        # ColVBERT / ColModernVBERT ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼é¸æŠ
        colbert_config = colbert_config or {}
        encoder_type = colbert_config.get('encoder_type', 'modern' if use_modern_vbert else 'standard')
        
        if encoder_type == 'modern' or use_modern_vbert:
            # ColModernVBERTï¼ˆSigLIPä½¿ç”¨ï¼‰
            self.colbert_encoder = ColModernVBERTEncoder(
                text_model_name=colbert_config.get('text_model', 'google/siglip-base-patch16-224'),
                vision_model_name=colbert_config.get('vision_model', 'google/siglip-base-patch16-224'),
                embedding_dim=colbert_config.get('embedding_dim', 768),
                use_cross_attention=colbert_config.get('use_cross_attention', True)
            )
            self.encoder_type = 'ColModernVBERT (SigLIP)'
        else:
            # å¾“æ¥ã®ColVBERTï¼ˆBLIPä½¿ç”¨ï¼‰
            self.colbert_encoder = ColVBERTEncoder(
                text_model_name=colbert_config.get('text_model', 'intfloat/multilingual-e5-large'),
                vision_model_name=colbert_config.get('vision_model', 'Salesforce/blip-image-captioning-base'),
                embedding_dim=colbert_config.get('embedding_dim', 768)
            )
            self.encoder_type = 'ColVBERT (BLIP)'
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ï¼ˆPDFå…ƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¸¡ã™ï¼‰
        visual_config = visual_config or {}
        self.visual_processor = VisualDocumentProcessor(
            ocr_config=visual_config.get('ocr_config'),
            pdf_dir=pdf_source_dir
        )
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.visual_documents: List[VisualDocument] = []
        self.visual_embeddings: Optional[np.ndarray] = None
        self.visual_index: Optional[faiss.Index] = None
        
        print(f"ğŸ–¼ï¸ Visual RAPTOR ColBERT initialized")
        print(f"   Encoder: {self.encoder_type}")
        print(f"   Device: {self.colbert_encoder.device}")
        if encoder_type == 'modern' or use_modern_vbert:
            print(f"   âœ¨ Using ColModernVBERT with SigLIP")
        else:
            print(f"   Text Model: {colbert_config.get('text_model', 'intfloat/multilingual-e5-large')}")
            print(f"   Vision Model: {colbert_config.get('vision_model', 'Salesforce/blip-image-captioning-base')}")
    
    def load_visual_documents(
        self,
        image_directory: str,
        supported_formats: List[str] = None
    ) -> List[VisualDocument]:
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’èª­ã¿è¾¼ã¿"""
        if supported_formats is None:
            supported_formats = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        
        image_dir = Path(image_directory)
        image_files = []
        
        for ext in supported_formats:
            image_files.extend(image_dir.glob(f"*{ext}"))
            image_files.extend(image_dir.glob(f"*{ext.upper()}"))
        
        print(f"ğŸ“¸ Processing {len(image_files)} visual documents...")
        
        visual_docs = []
        for i, image_path in enumerate(image_files, 1):
            if i % 20 == 0 or i == 1:
                print(f"   Progress: {i}/{len(image_files)}")
            visual_doc = self.visual_processor.process_visual_document(str(image_path))
            visual_docs.append(visual_doc)
        
        self.visual_documents = visual_docs
        print(f"âœ… Loaded {len(visual_docs)} visual documents")
        
        return visual_docs
    
    def build_visual_index(self) -> faiss.Index:
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        if not self.visual_documents:
            raise ValueError("ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        print(f"ğŸ” Building visual document index...")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’æº–å‚™
        texts = [doc.text_content for doc in self.visual_documents]
        images = []
        
        for doc in self.visual_documents:
            try:
                img = Image.open(doc.image_path).convert('RGB')
                images.append(img)
            except Exception as e:
                print(f"ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {doc.image_path}: {e}")
                # ãƒ€ãƒŸãƒ¼ç”»åƒã‚’ä½œæˆ
                images.append(Image.new('RGB', (224, 224), color='white'))
        
        # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
        embeddings = self.colbert_encoder.encode_multimodal(texts, images)
        embeddings_np = embeddings.detach().cpu().numpy()  # .detach()ã‚’è¿½åŠ 
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        embedding_dim = embeddings_np.shape[1]
        index = faiss.IndexFlatIP(embedding_dim)  # Inner Product for similarity
        index.add(embeddings_np.astype('float32'))
        
        self.visual_embeddings = embeddings_np
        self.visual_index = index
        
        print(f"âœ… Visual index built with {len(self.visual_documents)} documents")
        return index
    
    def search_visual_documents(
        self,
        query: str,
        query_image: Optional[str] = None,
        top_k: int = 5
    ) -> List[Tuple[VisualDocument, float]]:
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’æ¤œç´¢"""
        if self.visual_index is None:
            raise ValueError("ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ã‚¯ã‚¨ãƒªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
        if query_image:
            # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¯ã‚¨ãƒª
            query_img = Image.open(query_image).convert('RGB')
            query_embedding = self.colbert_encoder.encode_multimodal([query], [query_img])
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚¯ã‚¨ãƒª
            query_embedding = self.colbert_encoder.encode_text([query])
        
        query_vec = query_embedding.cpu().numpy().astype('float32')
        
        # æ¤œç´¢å®Ÿè¡Œ
        scores, indices = self.visual_index.search(query_vec, top_k)
        
        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.visual_documents):
                results.append((self.visual_documents[idx], float(score)))
        
        return results
    
    def create_hybrid_documents(self) -> List[Document]:
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’LangChainã®Documentã«å¤‰æ›"""
        hybrid_docs = []
        
        for i, visual_doc in enumerate(self.visual_documents):
            # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµåˆ
            content = f"ç”»åƒãƒ‘ã‚¹: {visual_doc.image_path}\n"
            if visual_doc.text_content:
                content += f"æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ:\n{visual_doc.text_content}\n"
            
            # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæƒ…å ±ã‚’è¿½åŠ 
            if visual_doc.layout_elements:
                content += f"ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦ç´ : {len(visual_doc.layout_elements)}å€‹ã®è¦ç´ ã‚’æ¤œå‡º\n"
                for elem in visual_doc.layout_elements:
                    content += f"- {elem['type']}: é¢ç©{elem['area']}\n"
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            metadata = {
                'source': visual_doc.image_path,
                'type': 'visual_document',
                'image_size': visual_doc.metadata.get('image_size'),
                'layout_elements_count': len(visual_doc.layout_elements),
                'visual_doc_index': i
            }
            
            doc = Document(page_content=content, metadata=metadata)
            hybrid_docs.append(doc)
        
        return hybrid_docs
    
    def build_hybrid_tree(
        self,
        text_documents: List[Document],
        visual_documents: List[VisualDocument] = None,
        save_dir: str = "saved_models/visual_raptor"
    ) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã®çµ±åˆãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰"""
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ã‚’Documentã«å¤‰æ›
        if visual_documents is None:
            visual_documents = self.visual_documents
        
        if visual_documents:
            hybrid_docs = self.create_hybrid_documents()
            all_documents = text_documents + hybrid_docs
        else:
            all_documents = text_documents
        
        print(f"ğŸŒ² Building hybrid tree with {len(text_documents)} text docs and {len(visual_documents)} visual docs")
        
        # è¦ªã‚¯ãƒ©ã‚¹ã®ãƒ„ãƒªãƒ¼æ§‹ç¯‰ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
        tree = self.build_disaster_tree(all_documents, save_dir)
        
        return tree
    
    def save_visual_components(self, save_dir: str):
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä¿å­˜"""
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        visual_docs_data = []
        for doc in self.visual_documents:
            visual_docs_data.append({
                'image_path': doc.image_path,
                'text_content': doc.text_content,
                'layout_elements': doc.layout_elements,
                'metadata': doc.metadata
            })
        
        with open(save_path / 'visual_documents.json', 'w', encoding='utf-8') as f:
            json.dump(visual_docs_data, f, ensure_ascii=False, indent=2)
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
        if self.visual_embeddings is not None:
            np.save(save_path / 'visual_embeddings.npy', self.visual_embeddings)
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        if self.visual_index is not None:
            faiss.write_index(self.visual_index, str(save_path / 'visual_index.faiss'))
        
        print(f"âœ… Visual components saved to {save_dir}")
    
    def load_visual_components(self, save_dir: str):
        """ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        save_path = Path(save_dir)
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        visual_docs_file = save_path / 'visual_documents.json'
        if visual_docs_file.exists():
            with open(visual_docs_file, 'r', encoding='utf-8') as f:
                visual_docs_data = json.load(f)
            
            self.visual_documents = []
            for data in visual_docs_data:
                visual_doc = VisualDocument(
                    image_path=data['image_path'],
                    text_content=data['text_content'],
                    layout_elements=data['layout_elements'],
                    metadata=data['metadata']
                )
                self.visual_documents.append(visual_doc)
        
        # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
        embeddings_file = save_path / 'visual_embeddings.npy'
        if embeddings_file.exists():
            self.visual_embeddings = np.load(embeddings_file)
        
        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        index_file = save_path / 'visual_index.faiss'
        if index_file.exists():
            self.visual_index = faiss.read_index(str(index_file))
        
        print(f"âœ… Visual components loaded from {save_dir}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    from langchain_ollama import OllamaEmbeddings, ChatOllama
    
    print("="*80)
    print("Visual RAPTOR ColBERT System")
    print("ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«æ–‡æ›¸å¯¾å¿œ éœ‡ç½æ•™è¨“ç¶™æ‰¿ã‚·ã‚¹ãƒ†ãƒ ")
    print("="*80)
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    embeddings = OllamaEmbeddings(
        model="mxbai-embed-large",
        base_url="http://localhost:11434"
    )
    llm = ChatOllama(
        model="granite-code:8b",
        temperature=0,
        base_url="http://localhost:11434",
        timeout=300
    )
    
    # VisualRAPTORColBERTåˆæœŸåŒ–
    visual_raptor = VisualRAPTORColBERT(
        embeddings_model=embeddings,
        llm=llm,
        colbert_config={
            'text_model': 'intfloat/multilingual-e5-large',
            'vision_model': 'Salesforce/blip2-opt-2.7b',
            'embedding_dim': 768
        },
        visual_config={
            'ocr_config': {
                'lang': 'jpn+eng',
                'config': '--psm 6'
            }
        },
        min_clusters=2,
        max_clusters=5,
        max_depth=3,
        chunk_size=500,
        chunk_overlap=100
    )
    
    print("âœ… Visual RAPTOR ColBERT ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")


if __name__ == "__main__":
    main()