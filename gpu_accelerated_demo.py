#!/usr/bin/env python3
"""
GPU-Accelerated Visual RAPTOR ColBERT Demo
GPUã‚’æ´»ç”¨ã—ãŸVisual RAPTOR ColBERTã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
"""

import torch
import torch.nn.functional as F
import numpy as np
import time
from pathlib import Path
from typing import Dict, Any, List, Tuple
import json

# LangChain imports
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain.schema import Document

print("ğŸ”§ GPU Configuration Check")
print("="*50)
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"Device count: {torch.cuda.device_count()}")

if torch.cuda.is_available():
    print(f"GPU device: {torch.cuda.get_device_name(0)}")
    print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print("âœ… GPU ready for acceleration")
else:
    print("âŒ GPU not available, using CPU")

# GPUè¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")


class GPUAcceleratedEmbeddings:
    """GPUåŠ é€ŸåŸ‹ã‚è¾¼ã¿ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, ollama_embeddings, batch_size=32):
        self.ollama_embeddings = ollama_embeddings
        self.batch_size = batch_size
        self.device = device
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨
        self.embedding_cache = {}
        
    def embed_documents_batch(self, texts: List[str]) -> torch.Tensor:
        """ãƒãƒƒãƒã§æ–‡æ›¸ã‚’åŸ‹ã‚è¾¼ã¿"""
        print(f"ğŸš€ GPU batch embedding for {len(texts)} documents...")
        
        embeddings_list = []
        
        for i in range(0, len(texts), self.batch_size):
            batch_texts = texts[i:i + self.batch_size]
            print(f"   Processing batch {i//self.batch_size + 1}/{(len(texts)-1)//self.batch_size + 1}")
            
            # ãƒãƒƒãƒã§OllamaåŸ‹ã‚è¾¼ã¿å–å¾—
            batch_embeddings = []
            for text in batch_texts:
                if text in self.embedding_cache:
                    embedding = self.embedding_cache[text]
                else:
                    embedding = self.ollama_embeddings.embed_query(text)
                    self.embedding_cache[text] = embedding
                batch_embeddings.append(embedding)
            
            # GPUãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
            batch_tensor = torch.tensor(batch_embeddings, dtype=torch.float32, device=self.device)
            embeddings_list.append(batch_tensor)
        
        # å…¨ãƒãƒƒãƒã‚’çµåˆ
        all_embeddings = torch.cat(embeddings_list, dim=0)
        
        print(f"âœ… Batch embedding completed: {all_embeddings.shape}")
        return all_embeddings
    
    def embed_query(self, query: str) -> torch.Tensor:
        """ã‚¯ã‚¨ãƒªã‚’åŸ‹ã‚è¾¼ã¿"""
        if query in self.embedding_cache:
            embedding = self.embedding_cache[query]
        else:
            embedding = self.ollama_embeddings.embed_query(query)
            self.embedding_cache[query] = embedding
            
        return torch.tensor(embedding, dtype=torch.float32, device=self.device)


class GPUAcceleratedSearch:
    """GPUåŠ é€Ÿæ¤œç´¢ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, gpu_embeddings):
        self.gpu_embeddings = gpu_embeddings
        self.document_embeddings = None
        self.documents = []
        
    def index_documents(self, documents: List[Dict], doc_texts: List[str]):
        """æ–‡æ›¸ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–"""
        print("ğŸ“š GPU-accelerated document indexing...")
        
        start_time = time.time()
        
        # GPU ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿
        self.document_embeddings = self.gpu_embeddings.embed_documents_batch(doc_texts)
        self.documents = documents
        
        # æ­£è¦åŒ–ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ç”¨ï¼‰
        self.document_embeddings = F.normalize(self.document_embeddings, p=2, dim=1)
        
        index_time = time.time() - start_time
        
        print(f"âœ… Indexing completed in {index_time:.2f}s")
        print(f"   Documents: {len(documents)}")
        print(f"   Embedding shape: {self.document_embeddings.shape}")
        print(f"   GPU memory used: {torch.cuda.memory_allocated()/1e9:.2f} GB")
        
        return index_time
    
    def search_gpu(self, query: str, top_k: int = 5) -> List[Tuple[Dict, float]]:
        """GPUåŠ é€Ÿæ¤œç´¢"""
        if self.document_embeddings is None:
            return []
        
        start_time = time.time()
        
        # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿
        query_embedding = self.gpu_embeddings.embed_query(query)
        query_embedding = F.normalize(query_embedding.unsqueeze(0), p=2, dim=1)
        
        # GPUä¸Šã§ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarities = torch.mm(query_embedding, self.document_embeddings.t()).squeeze(0)
        
        # Top-Kå–å¾—
        top_scores, top_indices = torch.topk(similarities, k=min(top_k, len(self.documents)))
        
        # CPUã«ç§»å‹•ã—ã¦çµæœä½œæˆ
        top_scores = top_scores.cpu().numpy()
        top_indices = top_indices.cpu().numpy()
        
        results = []
        for idx, score in zip(top_indices, top_scores):
            results.append((self.documents[idx], float(score)))
        
        search_time = time.time() - start_time
        
        return results, search_time


class GPUDisasterSearchDemo:
    """GPUç½å®³æ–‡æ›¸æ¤œç´¢ãƒ‡ãƒ¢"""
    
    def __init__(self):
        self.data_dir = Path("data/gpu_demo")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.ollama_embeddings = None
        self.llm = None
        self.gpu_embeddings = None
        self.gpu_search = None
        
    def initialize_models(self):
        """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        print("ğŸ”§ Initializing models...")
        
        # OllamaåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
        self.ollama_embeddings = OllamaEmbeddings(
            model="mxbai-embed-large",
            base_url="http://localhost:11434"
        )
        
        # LLMãƒ¢ãƒ‡ãƒ«
        self.llm = ChatOllama(
            model="granite-code:8b",
            temperature=0.1,
            base_url="http://localhost:11434",
            timeout=120
        )
        
        # GPUåŠ é€ŸåŸ‹ã‚è¾¼ã¿
        self.gpu_embeddings = GPUAcceleratedEmbeddings(
            self.ollama_embeddings,
            batch_size=16  # RTX 4060 Tiç”¨ã«èª¿æ•´
        )
        
        # GPUåŠ é€Ÿæ¤œç´¢
        self.gpu_search = GPUAcceleratedSearch(self.gpu_embeddings)
        
        print("âœ… Models initialized with GPU acceleration")
    
    def create_large_disaster_dataset(self, num_docs: int = 100) -> Tuple[List[Dict], List[str]]:
        """å¤§è¦æ¨¡ç½å®³æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        print(f"ğŸ“„ Creating large disaster dataset ({num_docs} documents)...")
        
        # ç½å®³ã‚«ãƒ†ã‚´ãƒªã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        disaster_templates = {
            "earthquake": {
                "title_templates": [
                    "åœ°éœ‡ç™ºç”Ÿæ™‚ã®é¿é›£æ‰‹é † #{id}",
                    "éœ‡åº¦{magnitude}ã®åœ°éœ‡å¯¾å¿œãƒãƒ‹ãƒ¥ã‚¢ãƒ« #{id}",
                    "åœ°éœ‡ã«ã‚ˆã‚‹å»ºç‰©è¢«å®³èª¿æŸ»å ±å‘Š #{id}",
                    "åœ°éœ‡é¿é›£æ‰€é‹å–¶ã‚¬ã‚¤ãƒ‰ #{id}"
                ],
                "content_templates": [
                    "åœ°éœ‡ç™ºç”Ÿæ™‚ã¯ç›´ã¡ã«æœºã®ä¸‹ã«èº«ã‚’éš ã—ã€æºã‚ŒãŒåã¾ã£ãŸã‚‰é¿é›£çµŒè·¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                    "éœ‡åº¦{magnitude}ä»¥ä¸Šã®åœ°éœ‡ã§ã¯å»ºç‰©å€’å£Šã®å±é™ºãŒã‚ã‚‹ãŸã‚ã€é€Ÿã‚„ã‹ã«å±‹å¤–ã«é¿é›£ã—ã¦ãã ã•ã„ã€‚",
                    "åœ°éœ‡å¾Œã¯ä½™éœ‡ã«æ³¨æ„ã—ã€ã‚¬ã‚¹ãƒ»é›»æ°—ã®å®‰å…¨ç¢ºèªã‚’è¡Œã£ã¦ã‹ã‚‰é¿é›£æ‰€ã«å‘ã‹ã£ã¦ãã ã•ã„ã€‚",
                    "åœ°éœ‡é¿é›£æ™‚ã¯é ­éƒ¨ã‚’ä¿è­·ã—ã€è½ä¸‹ç‰©ã«æ³¨æ„ã—ã¦é¿é›£ã—ã¦ãã ã•ã„ã€‚"
                ]
            },
            "tsunami": {
                "title_templates": [
                    "æ´¥æ³¢è­¦å ±ç™ºä»¤æ™‚ã®ç·Šæ€¥é¿é›£ #{id}",
                    "æ´¥æ³¢é¿é›£ãƒ“ãƒ«æŒ‡å®šæ–½è¨­ #{id}",
                    "æ´¥æ³¢è¢«å®³æƒ³å®šèª¿æŸ» #{id}",
                    "æ´¥æ³¢é¿é›£ã‚¿ãƒ¯ãƒ¼é‹ç”¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ« #{id}"
                ],
                "content_templates": [
                    "æ´¥æ³¢è­¦å ±ç™ºä»¤æ™‚ã¯ç›´ã¡ã«é«˜å°ã¾ãŸã¯3éšå»ºã¦ä»¥ä¸Šã®é ‘ä¸ˆãªå»ºç‰©ã«é¿é›£ã—ã¦ãã ã•ã„ã€‚",
                    "æµ·å²¸ã‹ã‚‰æœ€ä½2kmä»¥ä¸Šå†…é™¸ã«é¿é›£ã—ã€æ¨™é«˜20mä»¥ä¸Šã®å ´æ‰€ã‚’ç›®æŒ‡ã—ã¦ãã ã•ã„ã€‚",
                    "æ´¥æ³¢ã¯ç¬¬ä¸€æ³¢ã‚ˆã‚Šç¬¬äºŒæ³¢ä»¥é™ãŒé«˜ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€è­¦å ±è§£é™¤ã¾ã§é¿é›£ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚",
                    "è‡ªå‹•è»Šã§ã®é¿é›£ã¯æ¸‹æ»ã®åŸå› ã¨ãªã‚‹ãŸã‚ã€å¾’æ­©ã§ã®é¿é›£ã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚"
                ]
            },
            "fire": {
                "title_templates": [
                    "ç«ç½ç™ºç”Ÿæ™‚ã®åˆæœŸæ¶ˆç« #{id}",
                    "å¤§è¦æ¨¡ç«ç½é¿é›£è¨ˆç”» #{id}",
                    "æ¶ˆé˜²è¨­å‚™ç‚¹æ¤œãƒãƒ‹ãƒ¥ã‚¢ãƒ« #{id}",
                    "ç«ç½äºˆé˜²å¯¾ç­–ã‚¬ã‚¤ãƒ‰ #{id}"
                ],
                "content_templates": [
                    "ç«ç½ç™ºè¦‹æ™‚ã¯å¤§å£°ã§å‘¨å›²ã«çŸ¥ã‚‰ã›ã€119ç•ªé€šå ±å¾Œã«åˆæœŸæ¶ˆç«ã‚’è©¦ã¿ã¦ãã ã•ã„ã€‚",
                    "ç…™ã«ã‚ˆã‚‹è¦–ç•Œä¸è‰¯æ™‚ã¯å§¿å‹¢ã‚’ä½ãã—ã€å£ä¼ã„ã«é¿é›£çµŒè·¯ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚",
                    "æ¶ˆç«å™¨ã¯ç«å…ƒã‹ã‚‰2-3mé›¢ã‚ŒãŸé¢¨ä¸Šã‹ã‚‰ä½¿ç”¨ã—ã€ç‡ƒç„¼ç‰©ã®æ ¹å…ƒã‚’ç‹™ã£ã¦ãã ã•ã„ã€‚",
                    "é¿é›£æ™‚ã¯ã‚¨ãƒ¬ãƒ™ãƒ¼ã‚¿ãƒ¼ä½¿ç”¨ã‚’é¿ã‘ã€éšæ®µã‚’ä½¿ç”¨ã—ã¦é¿é›£ã—ã¦ãã ã•ã„ã€‚"
                ]
            },
            "flood": {
                "title_templates": [
                    "æ°´å®³æ™‚ã®é¿é›£è¡Œå‹• #{id}",
                    "æ²³å·æ°¾æ¿«å¯¾ç­–ãƒãƒ‹ãƒ¥ã‚¢ãƒ« #{id}",
                    "å†…æ°´æ°¾æ¿«é¿é›£ã‚¬ã‚¤ãƒ‰ #{id}",
                    "åœŸç ‚ç½å®³è­¦æˆ’æƒ…å ± #{id}"
                ],
                "content_templates": [
                    "æ°´å®³è­¦å ±ç™ºä»¤æ™‚ã¯ä½åœ°ã‹ã‚‰ã®é¿é›£ã‚’é–‹å§‹ã—ã€2éšä»¥ä¸Šã¸ã®å‚ç›´é¿é›£ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
                    "è†ä¸Šã¾ã§æµ¸æ°´ã—ãŸé“è·¯ã§ã¯æ­©è¡Œå›°é›£ã¨ãªã‚‹ãŸã‚ã€ç„¡ç†ãªç§»å‹•ã¯é¿ã‘ã¦ãã ã•ã„ã€‚",
                    "åœŸç ‚ç½å®³è­¦æˆ’åŒºåŸŸã§ã¯é™é›¨é‡ã«æ³¨æ„ã—ã€æ—©æœŸé¿é›£ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚",
                    "æ²³å·ã®å¢—æ°´æ™‚ã¯æ©‹æ¢ä»˜è¿‘ã¸ã®ç«‹ã¡å…¥ã‚Šã‚’é¿ã‘ã€å®‰å…¨ãªé¿é›£çµŒè·¯ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
                ]
            }
        }
        
        documents = []
        doc_texts = []
        
        categories = list(disaster_templates.keys())
        
        for i in range(num_docs):
            # ã‚«ãƒ†ã‚´ãƒªã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            category = categories[i % len(categories)]
            template_data = disaster_templates[category]
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨å†…å®¹ã‚’ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ
            title_template = np.random.choice(template_data["title_templates"])
            content_template = np.random.choice(template_data["content_templates"])
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç½®æ›
            magnitude = np.random.randint(4, 8)
            title = title_template.format(id=i+1, magnitude=magnitude)
            content = content_template.format(magnitude=magnitude)
            
            # è¿½åŠ è©³ç´°æƒ…å ±
            additional_info = [
                f"ç™ºè¡Œæ—¥: 2024å¹´{np.random.randint(1,13):02d}æœˆ{np.random.randint(1,29):02d}æ—¥",
                f"æ‹…å½“éƒ¨ç½²: {category.title()}å¯¾ç­–èª²",
                f"æ–‡æ›¸ç•ªå·: {category.upper()}-{i+1:04d}",
                f"é‡è¦åº¦: {'é«˜' if i % 3 == 0 else 'ä¸­' if i % 3 == 1 else 'ä½'}"
            ]
            
            full_content = content + "\n\n" + "\n".join(additional_info)
            
            doc = {
                'doc_id': f"doc_{i+1:04d}",
                'title': title,
                'content': full_content,
                'category': category,
                'metadata': {
                    'doc_number': i+1,
                    'category': category,
                    'importance': additional_info[3].split(': ')[1],
                    'creation_date': additional_info[0].split(': ')[1]
                }
            }
            
            documents.append(doc)
            doc_texts.append(f"{title}\n{full_content}")
        
        print(f"âœ… Created {len(documents)} disaster documents")
        return documents, doc_texts
    
    def run_gpu_performance_test(self, documents: List[Dict], doc_texts: List[str]):
        """GPUæ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸš€ GPU Performance Test")
        print("="*50)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚é–“æ¸¬å®š
        index_time = self.gpu_search.index_documents(documents, doc_texts)
        
        # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
        test_queries = [
            "åœ°éœ‡ãŒç™ºç”Ÿã—ãŸã¨ãã®é¿é›£æ–¹æ³•",
            "æ´¥æ³¢è­¦å ±æ™‚ã®ç·Šæ€¥å¯¾å¿œ",
            "ç«ç½ç™ºç”Ÿæ™‚ã®åˆæœŸæ¶ˆç«æ‰‹é †",
            "æ°´å®³æ™‚ã®é¿é›£è¡Œå‹•",
            "ç·Šæ€¥æ™‚ã®é€£çµ¡ä½“åˆ¶",
            "é¿é›£æ‰€ã§ã®ç”Ÿæ´»ã‚¬ã‚¤ãƒ‰",
            "ç½å®³å‚™è“„å“ã®æº–å‚™",
            "å¿œæ€¥æ‰‹å½“ã®åŸºæœ¬çŸ¥è­˜",
            "åœé›»æ™‚ã®å¯¾å¿œç­–",
            "é«˜é½¢è€…ã¸ã®ç½å®³æ”¯æ´"
        ]
        
        print(f"\nğŸ” Running {len(test_queries)} search queries...")
        
        search_times = []
        all_results = {}
        
        for i, query in enumerate(test_queries):
            print(f"   Query {i+1}: {query}")
            
            results, search_time = self.gpu_search.search_gpu(query, top_k=5)
            search_times.append(search_time)
            
            print(f"      Search time: {search_time:.4f}s")
            print(f"      Top result: {results[0][0]['title']} (score: {results[0][1]:.4f})")
            
            all_results[f"query_{i+1}"] = {
                'query': query,
                'search_time': search_time,
                'results': [
                    {
                        'title': result[0]['title'],
                        'category': result[0]['category'],
                        'score': result[1]
                    }
                    for result in results
                ]
            }
        
        # æ€§èƒ½çµ±è¨ˆ
        performance_stats = {
            'total_documents': len(documents),
            'total_queries': len(test_queries),
            'index_build_time': index_time,
            'avg_search_time': np.mean(search_times),
            'min_search_time': np.min(search_times),
            'max_search_time': np.max(search_times),
            'total_search_time': np.sum(search_times),
            'gpu_memory_allocated': torch.cuda.memory_allocated() / 1e9,
            'gpu_memory_cached': torch.cuda.memory_reserved() / 1e9
        }
        
        print(f"\nğŸ“Š Performance Summary:")
        print(f"   Index build time: {performance_stats['index_build_time']:.2f}s")
        print(f"   Average search time: {performance_stats['avg_search_time']:.4f}s")
        print(f"   Search throughput: {len(test_queries)/performance_stats['total_search_time']:.2f} queries/sec")
        print(f"   GPU memory used: {performance_stats['gpu_memory_allocated']:.2f} GB")
        
        return all_results, performance_stats
    
    def save_results(self, results: Dict, stats: Dict):
        """çµæœã‚’ä¿å­˜"""
        output_data = {
            'performance_stats': stats,
            'search_results': results,
            'gpu_info': {
                'device_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',
                'cuda_version': torch.version.cuda,
                'pytorch_version': torch.__version__
            }
        }
        
        output_file = self.data_dir / "gpu_performance_results.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Results saved to {output_file}")
    
    def run_complete_demo(self):
        """å®Œå…¨GPUãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ"""
        print("="*80)
        print("ğŸš€ GPU-Accelerated Visual RAPTOR Demo")
        print("GPUåŠ é€Ÿç½å®³æ–‡æ›¸æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ")
        print("="*80)
        
        try:
            # 1. ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
            self.initialize_models()
            
            # 2. å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            documents, doc_texts = self.create_large_disaster_dataset(200)  # RTX 4060 Tiç”¨
            
            # 3. GPUæ€§èƒ½ãƒ†ã‚¹ãƒˆ
            results, stats = self.run_gpu_performance_test(documents, doc_texts)
            
            # 4. çµæœä¿å­˜
            self.save_results(results, stats)
            
            print("\n" + "="*80)
            print("ğŸ‰ GPU Demo Completed Successfully!")
            print("="*80)
            print(f"GPU Acceleration: âœ… ENABLED")
            print(f"Documents processed: {stats['total_documents']}")
            print(f"Search throughput: {stats['total_queries']/stats['total_search_time']:.2f} queries/sec")
            print(f"GPU efficiency: {stats['gpu_memory_allocated']:.2f} GB used")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ GPU demo failed: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    if not torch.cuda.is_available():
        print("âŒ CUDA is not available. Please check your GPU setup.")
        return False
    
    demo = GPUDisasterSearchDemo()
    success = demo.run_complete_demo()
    
    if success:
        print("\nğŸš€ GPU is now actively utilized!")
        print("Your RTX 4060 Ti is accelerating:")
        print("  - Batch document embedding")
        print("  - Parallel similarity computation")
        print("  - High-throughput search operations")
        print("  - Large-scale disaster document processing")
    
    return success


if __name__ == "__main__":
    main()